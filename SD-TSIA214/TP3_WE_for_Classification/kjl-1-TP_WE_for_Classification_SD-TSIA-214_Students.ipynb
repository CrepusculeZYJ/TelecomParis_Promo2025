{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9af7be",
   "metadata": {},
   "source": [
    "# TP : Word Embeddings for Classification\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "Explore the various way to represent textual data by applying them to a relatively small French classification dataset based on professionnal certification titles - **RNCP** - and evaluate how they perform on the classification task. \n",
    "1. Using what we have previously seen, pre-process the data: clean it, obtain an appropriate vocabulary.\n",
    "2. Obtain representations: any that will allow us to obtain a vector representation of each document is appropriate.\n",
    "    - Symbolic: **BoW, TF-IDF**\n",
    "    - Dense document representations: via **Topic Modeling: LSA, LDA**\n",
    "    - Dense word representations: **SVD-reduced PPMI, Word2vec, GloVe**\n",
    "        - For these, you will need to implement a **function aggregating word representations into document representations**\n",
    "3. Perform classification: we can make things simple and only use a **logistic regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7af15",
   "metadata": {},
   "source": [
    "## Necessary dependancies\n",
    "\n",
    "We will need the following packages:\n",
    "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
    "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
    "- Gensim: https://radimrehurek.com/gensim/\n",
    "\n",
    "These are available with Anaconda: https://anaconda.org/anaconda/nltk and https://anaconda.org/anaconda/scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079ef84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import re \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import gzip\n",
    "pp = pprint.PrettyPrinter(indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a147280",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Let's load the data: take a first look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a5e3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Categorie                                text_certifications\n",
      "0          1  Responsable de chantiers de bûcheronnage manue...\n",
      "1          1  Responsable de chantiers de bûcheronnage manue...\n",
      "2          1                                 Travaux forestiers\n",
      "3          1                                              Forêt\n",
      "4          1                                              Forêt\n"
     ]
    }
   ],
   "source": [
    "with open(\"rncp.csv\", encoding='utf-8') as f:\n",
    "    rncp = pd.read_csv(f, na_filter=False)\n",
    "\n",
    "print(rncp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b21e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Categorie' 'text_certifications']\n"
     ]
    }
   ],
   "source": [
    "print(rncp.columns.values)\n",
    "texts = rncp.loc[:,'text_certifications'].astype('str').tolist()\n",
    "labels = rncp.loc[:,'Categorie'].astype('str').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fa04f",
   "metadata": {},
   "source": [
    "You can see that the first column is the category, the second the title of the certification. Let's get the category names for clarity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9293f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "Categories = [\"1-environnement\",\n",
    "              \"2-defense\",\n",
    "              \"3-patrimoine\",\n",
    "              \"4-economie\",\n",
    "              \"5-recherche\",\n",
    "              \"6-nautisme\",\n",
    "              \"7-aeronautique\",\n",
    "              \"8-securite\",\n",
    "              \"9-multimedia\",\n",
    "              \"10-humanitaire\",\n",
    "              \"11-nucleaire\",\n",
    "              \"12-enfance\",\n",
    "              \"13-saisonnier\",\n",
    "              \"14-assistance\",\n",
    "              \"15-sport\",\n",
    "              \"16-ingenierie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2abb70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  'Responsable de chantiers de bûcheronnage manuel et de débardage',\n",
      "   'Responsable de chantiers de bûcheronnage manuel et de sylviculture',\n",
      "   'Travaux forestiers',\n",
      "   'Forêt',\n",
      "   'Forêt',\n",
      "   'Responsable de chantiers forestiers',\n",
      "   'Diagnostic et taille des arbres',\n",
      "   'option Chef d’entreprise ou OHQ en travaux forestiers, spécialité '\n",
      "   'abattage-façonnage',\n",
      "   'option Chef d’entreprise ou OHQ en travaux forestiers, spécialité '\n",
      "   'débardage',\n",
      "   'Gestion et conduite de chantiers forestiers']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9599792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 47156\n"
     ]
    }
   ],
   "source": [
    "# This number of documents may be high for some computers: we can select a fraction of them (here, one in k)\n",
    "# Use an even number to keep the same number of positive and negative reviews\n",
    "k = 2\n",
    "train_texts = texts\n",
    "train_labels = labels\n",
    "texts_reduced = train_texts[0::k]\n",
    "labels_reduced = train_labels[0::k]\n",
    "\n",
    "print('Number of documents:', len(texts_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd53e33",
   "metadata": {},
   "source": [
    "Use the function ```train_test_split```from ```sklearn``` function to set aside test data that you will use during the lab. Make it one fifth of the data you have currently.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5be9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts_reduced, labels_reduced, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbce6d4",
   "metadata": {},
   "source": [
    "## 1 - Document Preprocessing\n",
    "\n",
    "You should use a pre-processing function you can apply to the raw text before any other processing (*i.e*, tokenization and obtaining representations). Some pre-processing can also be tied with the tokenization (*i.e*, removing stop words). Complete the following function, using the appropriate ```nltk``` tools. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b4a144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GAALOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def pre_process(texts):\n",
    "    # lowercase\n",
    "    texts = [text.lower() for text in texts]\n",
    "    # remove puntuation\n",
    "    texts = [re.sub(r'[^\\w\\s]', '', text) for text in texts]\n",
    "    # tokenization\n",
    "    texts_token = [nltk.word_tokenize(text) for text in texts]\n",
    "    # stemming\n",
    "    stemmer = nltk.SnowballStemmer('french')\n",
    "    texts_stem = [[stemmer.stem(word) for word in text] for text in texts_token]\n",
    "    # remove stopwords\n",
    "    stop_words = set(nltk.corpus.stopwords.words('french'))\n",
    "    texts_stop = [[word for word in text if word not in stop_words] for text in texts_stem]\n",
    "    return texts_stop\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632db6f",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b33d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data and apply the appropriate pre-processing\n",
    "train_texts_clean = pre_process(train_texts)\n",
    "test_texts_clean = pre_process(test_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4140f1",
   "metadata": {},
   "source": [
    "Now that the data is cleaned, the first step we will follow is to pick a common vocabulary that we will use for every representations we obtain in this lab. **Use the code of the previous lab to create a vocabulary.**\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6910a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(corpus, count_threshold=0, voc_threshold=0):\n",
    "    \"\"\"    \n",
    "    Function using word counts to build a vocabulary - can be improved with a second parameter for \n",
    "    setting a frequency threshold\n",
    "    Params:\n",
    "        corpus (list of strings): corpus of sentences\n",
    "        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n",
    "        voc_threshold (int): maximum size of the vocabulary. Use \"0\" to indicate no limit \n",
    "    Returns:\n",
    "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
    "                                 values: indexes corresponding to each word sorted by frequency   \n",
    "        vocabulary_word_counts (dictionary): keys: list of distinct words across the corpus\n",
    "                                             values: corresponding counts of words in the corpus\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    for sent in corpus:\n",
    "        for word in sent.split():\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    filtered_word_counts = {word:count for word,count in word_counts.items() if count > count_threshold} # Filter according to count_threhshold        \n",
    "    sorted_words = sorted(filtered_word_counts, key=filtered_word_counts.get, reverse=True) # Extract the words according to frequency\n",
    "    filtered_words = sorted_words[:voc_threshold] if voc_threshold>0 else sorted_words # Remove the words above voc-threshold\n",
    "    words = filtered_words + [\"UNK\"] # Add UNK\n",
    "    vocabulary = {word: i for i, word in enumerate(words)} # Create vocabulary from \"words\"\n",
    "    return vocabulary, {word: filtered_word_counts.get(word, 0) for word in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "716e8964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spécial': 0, 'mention': 1, 'scienc': 2, 'diplôm': 3, 'sant': 4, 'technolog': 5, 'ingénieur': 6, 'gestion': 7, 'manag': 8, 'national': 9, 'domain': 10, 'droit': 11, 'mast': 12, 'informat': 13, 'system': 14, 'industriel': 15, 'professionnel': 16, 'gen': 17, 'supérieur': 18, 'social': 19, 'méti': 20, 'option': 21, 'commun': 22, 'langu': 23, 'humain': 24, 'lunivers': 25, 'art': 26, 'appliqu': 27, 'développ': 28, 'lecol': 29, 'final': 30, 'product': 31, 'ingénier': 32, 'linstitut': 33, 'respons': 34, 'econom': 35, 'fich': 36, 'projet': 37, 'recherch': 38, 'polytechn': 39, 'économ': 40, 'lettr': 41, 'environ': 42, 'matérial': 43, 'réseau': 44, 'international': 45, 'sécur': 46, 'lécol': 47, 'biolog': 48, 'physiqu': 49, 'mécan': 50, 'techniqu': 51, 'marketing': 52, 'lenviron': 53, 'chim': 54, 'qualit': 55, 'organis': 56, 'industr': 57, 'licenc': 58, 'chef': 59, 'mathémat': 60, 'commerc': 61, 'mainten': 62, 'activ': 63, 'télécommun': 64, 'expert': 65, 'dingénieur': 66, 'numer': 67, 'universitair': 68, 'entrepris': 69, 'histoir': 70, 'dinform': 71, 'concept': 72, 'technicien': 73, 'aménag': 74, 'public': 75, 'électron': 76, 'administr': 77, 'produit': 78, 'agroalimentair': 79, 'strateg': 80, 'bât': 81, 'commercial': 82, 'durabl': 83, 'risqu': 84, 'ressourc': 85, 'partenariat': 86, 'cqp': 87, 'automat': 88, 'logiciel': 89, 'électr': 90, 'cultur': 91, 'logist': 92, 'territoir': 93, 'charg': 94, 'biotechnolog': 95, 'patrimoin': 96, 'protect': 97, 'construct': 98, 'proced': 99, 'servic': 100, 'sport': 101, 'lill': 102, 'aliment': 103, 'contrôl': 104, 'linform': 105, 'conservatoir': 106, 'polit': 107, 'chimiqu': 108, 'traval': 109, 'affair': 110, 'culturel': 111, 'étranger': 112, 'énerget': 113, 'énerg': 114, 'civil': 115, 'civilis': 116, 'terr': 117, 'assist': 118, 'géograph': 119, 'litii': 120, 'innov': 121, 'valoris': 122, 'littératur': 123, 'anim': 124, 'multimédi': 125, 'publiqu': 126, 'dan': 127, 'inform': 128, 'agent': 129, 'paris': 130, 'format': 131, 'analys': 132, 'priv': 133, 'léduc': 134, 'lindustr': 135, 'bordeau': 136, 'digital': 137, 'pharmaceut': 138, 'tourism': 139, 'agronom': 140, 'polytech': 141, 'degr': 142, 'sportiv': 143, 'electron': 144, 'toulous': 145, 'européen': 146, 'détud': 147, 'programm': 148, 'lingénieur': 149, 'financ': 150, 'daffair': 151, 'instrument': 152, 'integr': 153, 'dentrepris': 154, 'perform': 155, 'étud': 156, 'climat': 157, 'conseil': 158, 'opérationnel': 159, 'societ': 160, 'trait': 161, 'sportif': 162, 'méthod': 163, 'franc': 164, 'montpelli': 165, 'ecol': 166, 'parcour': 167, 'transport': 168, 'psycholog': 169, 'grand': 170, 'jeuness': 171, 'statist': 172, 'ren': 173, 'energ': 174, 'littoral': 175, 'spécialis': 176, 'don': 177, 'naturel': 178, 'vent': 179, 'création': 180, 'brevet': 181, 'paysag': 182, 'transform': 183, 'design': 184, 'eau': 185, 'populair': 186, 'vi': 187, 'avanc': 188, 'group': 189, 'lenseign': 190, 'titr': 191, 'web': 192, 'développeur': 193, 'etranger': 194, 'person': 195, 'prévent': 196, 'territorial': 197, 'lyon': 198, 'espac': 199, 'etud': 200, 'concepteur': 201, 'alimentair': 202, 'electr': 203, 'bois': 204, 'strasbourg': 205, 'mod': 206, 'végétal': 207, 'lentrepris': 208, 'oper': 209, 'structur': 210, 'lorrain': 211, 'embarqu': 212, 'rural': 213, 'premi': 214, 'aéronaut': 215, 'artist': 216, 'centr': 217, 'min': 218, 'hygien': 219, 'conduit': 220, 'lénerg': 221, 'modélis': 222, 'achat': 223, 'grenobl': 224, 'intelligent': 225, 'intervent': 226, 'mécatron': 227, 'textil': 228, 'bretagn': 229, 'commercialis': 230, 'déquip': 231, 'environnemental': 232, 'sciencestechnologiessant': 233, 'composit': 234, 'ouvri': 235, 'sociolog': 236, 'leau': 237, 'financi': 238, 'dart': 239, 'urban': 240, 'médiat': 241, 'enseign': 242, 'cellulair': 243, 'lart': 244, 'nutrit': 245, 'complex': 246, 'régional': 247, 'directeur': 248, 'travail': 249, 'loisir': 250, 'réalis': 251, 'robot': 252, 'médi': 253, 'relat': 254, 'archéolog': 255, 'microbiolog': 256, 'philosoph': 257, 'mesur': 258, 'côt': 259, 'urbain': 260, 'automatis': 261, 'detat': 262, 'imag': 263, 'éduc': 264, 'bien': 265, 'gestionnair': 266, 'univers': 267, 'install': 268, 'renouvel': 269, 'maîtris': 270, '1': 271, 'nant': 272, 'distribu': 273, 'agricol': 274, 'écol': 275, 'collect': 276, 'nucléair': 277, 'physiolog': 278, 'médic': 279, 'mer': 280, 'comptabl': 281, 'mati': 282, 'analyt': 283, 'dinformat': 284, 'meilleur': 285, 'linnov': 286, 'viv': 287, 'moléculair': 288, 'audit': 289, 'délectron': 290, 'pratiqu': 291, 'miag': 292, 'ergonom': 293, 'général': 294, 'journal': 295, 'sanitair': 296, 'mobil': 297, 'haut': 298, 'ecolog': 299, 'langag': 300, 'convent': 301, 'droiteconomiegest': 302, 'écolog': 303, 'rouen': 304, 'linformat': 305, 'processus': 306, 'criminel': 307, 'planet': 308, 'mond': 309, 'restaur': 310, 'nouvel': 311, 'architectur': 312, 'expertis': 313, 'energet': 314, 'pluritechn': 315, 'télécom': 316, 'jurid': 317, 'minestélécom': 318, 'limog': 319, 'métrolog': 320, 'optiqu': 321, 'patholog': 322, 'deuxiem': 323, 'milieux': 324, 'march': 325, 'plastiqu': 326, 'dynam': 327, 'mis': 328, 'dexploit': 329, 'thermiqu': 330, 'ii': 331, 'fondamental': 332, 'biodivers': 333, 'acoust': 334, 'cognit': 335, 'stap': 336, 'linternational': 337, 'marin': 338, 'équip': 339, 'sorbon': 340, 'tourist': 341, 'form': 342, 'optimis': 343, 'coordon': 344, 'internet': 345, 'interculturel': 346, 'business': 347, 'conducteur': 348, 'i': 349, 'entrepreneuriat': 350, 'associ': 351, 'animal': 352, 'bas': 353, 'local': 354, 'spectacl': 355, 'scientif': 356, 'décis': 357, 'diagnostic': 358, 'formul': 359, 'géolog': 360, 'limag': 361, 'laliment': 362, 'secteur': 363, 'dazur': 364, 'jurist': 365, 'global': 366, 'central': 367, 'audiovisuel': 368, 'graphiqu': 369, 'savoi': 370, 'écosystem': 371, 'st': 372, 'anglais': 373, 'coordin': 374, 'vis': 375, 'daixmarseil': 376, 'dappliqu': 377, 'génom': 378, 'clermontferrand': 379, 'loir': 380, 'moniteur': 381, 'btp': 382, 'bioinformat': 383, 'linguist': 384, 'immobili': 385, 'pierr': 386, 'plasturg': 387, 'mécanicien': 388, 'poiti': 389, 'cliniqu': 390, 'document': 391, 'laménag': 392, 'grad': 393, 'educ': 394, 'dirig': 395, 'adapt': 396, 'bcpp': 397, 'maritim': 398, 'musiqu': 399, 'mar': 400, 'espagnol': 401, 'génet': 402, 'institu': 403, 'hôteller': 404, 'dactiv': 405, 'sûret': 406, 'biochim': 407, 'automobil': 408, 'action': 409, 'lélectron': 410, 'cur': 411, 'chambéry': 412, 'documentair': 413, 'cooper': 414, 'meef': 415, 'direct': 416, 'allemand': 417, 'dat': 418, 'leduc': 419, 'géomat': 420, 'livr': 421, 'connaiss': 422, 'géoscienc': 423, 'class': 424, 'artificiel': 425, 'forêt': 426, 'second': 427, 'vill': 428, 'déchet': 429, 'client': 430, 'intérieur': 431, 'léconom': 432, 'toxicolog': 433, 'anthropolog': 434, 'cybersécur': 435, 'lign': 436, 'modern': 437, 'thérapeut': 438, 'italien': 439, 'carri': 440, 'pay': 441, 'specialit': 442, 'ingenier': 443, 'physicochim': 444, 'solidair': 445, 'biomédical': 446, 'caen': 447, 'négoci': 448, 'signal': 449, 'chain': 450, 'contemporain': 451, 'and': 452, 'tour': 453, 'informationcommun': 454, 'moteur': 455, 'gouvern': 456, 'organ': 457, 'assur': 458, 'dépollu': 459, 'agricultur': 460, 'chanti': 461, 'b': 462, 'véhicul': 463, 'tvrn': 464, 'solut': 465, 'calcul': 466, 'lédit': 467, 'spatial': 468, 'rédact': 469, 'lagricultur': 470, 'biologiesant': 471, 'habill': 472, 'ordin': 473, 'traduct': 474, 'judiciair': 475, 'bibliothequ': 476, 'médical': 477, 'incend': 478, '2': 479, 'marseil': 480, 'dintervent': 481, 'laboratoir': 482, 'dales': 483, 'encadr': 484, 'troy': 485, 'dunit': 486, 'consult': 487, 'sit': 488, 'pétroli': 489, 'polyvalent': 490, 'technicocommercial': 491, 'iledefr': 492, 'banqu': 493, 'perfection': 494, 'in': 495, 'cathol': 496, 'montagn': 497, 'dhydraul': 498, 'décisionnel': 499, 'milieu': 500, 'didact': 501, 'établ': 502, 'compar': 503, 'expérimental': 504, 'pilotag': 505, 'human': 506, 'exploit': 507, 'vin': 508, 'cosmet': 509, 'visuel': 510, 'saintétien': 511, 'supply': 512, 'dou': 513, 'domainesciencestechnologiessant': 514, 'fabriqu': 515, 'analyst': 516, 'school': 517, 'aquat': 518, 'disciplin': 519, 'attach': 520, 'moyen': 521, 'luniver': 522, 'coach': 523, 'dun': 524, 'lutt': 525, 'pétrol': 526, 'architect': 527, 'ecommerc': 528, 'dagricultur': 529, 'detud': 530, 'pénal': 531, 'prof': 532, 'dessin': 533, 'dorléan': 534, 'dopal': 535, 'pyrotechn': 536, 'atlant': 537, 'petit': 538, 'capitain': 539, 'po': 540, 'organisationnel': 541, 'infrastructur': 542, 'confer': 543, 'surveil': 544, 'nancy': 545, 'lux': 546, 'lenerg': 547, 'certificat': 548, 'a': 549, 'chaîn': 550, 'polymer': 551, 'brest': 552, 'interact': 553, 'clermont': 554, '3': 555, 'capteur': 556, 'foresti': 557, 'méditerran': 558, 'command': 559, 'compétent': 560, 'glac': 561, 'efficac': 562, 'stpe': 563, 'amélior': 564, 'ecoconcept': 565, 'cadr': 566, 'belfortmontbéliard': 567, 'danalys': 568, 'molécul': 569, '3d': 570, 'naval': 571, 'contr': 572, 'fiscal': 573, 'transfrontali': 574, 'institut': 575, 'microélectron': 576, 'maintenicien': 577, 'etou': 578, 'interpret': 579, 'sinistr': 580, 'accompagn': 581, 'cqpm': 582, 'propuls': 583, 'synthes': 584, 'docu': 585, 'pau': 586, 'sol': 587, 'europ': 588, 'cellul': 589, 'man': 590, 'physiopatholog': 591, 'sud': 592, 'ecosystem': 593, 'danger': 594, 'reim': 595, 'délectrotechn': 596, 'détat': 597, 'fonction': 598, 'pme': 599, 'ferroviair': 600, 'chaudronner': 601, 'lespac': 602, 'quantit': 603, 'évolu': 604, 'enspm': 605, 'bioscienc': 606, 'géotechn': 607, 'quart': 608, 'diagnostiqueur': 609, 'matériel': 610, 'conserv': 611, 'dingénier': 612, 'agroscient': 613, 'décor': 614, 'biomolécul': 615, 'laéronaut': 616, 'lurban': 617, 'lingénier': 618, 'nautiqu': 619, 'saintetien': 620, 'silico': 621, 'infograph': 622, 'topograph': 623, 'lagroalimentair': 624, 'nanotechnolog': 625, 'sciencestechnologiesant': 626, 'dijon': 627, 'histor': 628, 'défens': 629, 'bioindustr': 630, 'paysager': 631, 'lénerget': 632, 'fili': 633, 'unit': 634, 'evolu': 635, 'miash': 636, 'répart': 637, 'firm': 638, 'vibrat': 639, 'denseign': 640, 'événementiel': 641, 'anglophon': 642, 'inp': 643, 'aérien': 644, 'condition': 645, 'handicap': 646, 'xi': 647, 'surfac': 648, 'cinem': 649, 'non': 650, 'archiv': 651, 'agrosystem': 652, 'classiqu': 653, 'mentionméti': 654, 'e': 655, 'pharmacolog': 656, 'géophys': 657, 'consomm': 658, 'alsac': 659, 'vieil': 660, 'vétérinair': 661, 'imager': 662, 'cheff': 663, 'peintr': 664, 'diffus': 665, 'daccueil': 666, 'guid': 667, 'rapproch': 668, 'ameubl': 669, 'gaz': 670, 'médicosocial': 671, 'aquitain': 672, 'directeurtric': 673, 'spiritu': 674, 'lensilensc': 675, 'golf': 676, 'linternet': 677, 'russ': 678, 'facteur': 679, 'ensib': 680, 'écoconcept': 681, 'air': 682, 'fluid': 683, 'contenti': 684, 'patrimonial': 685, 'photon': 686, 'littérair': 687, 'édit': 688, 'cy': 689, 'communic': 690, 'nuisanc': 691, 'créateur': 692, 'pilot': 693, 'secour': 694, 'contrôleur': 695, 'popul': 696, 'agroressourc': 697, 'material': 698, 'dateli': 699, 'lanalys': 700, 'bm': 701, 'iii': 702, 'football': 703, 'emballag': 704, 'emploi': 705, 'enseeiht': 706, 'théor': 707, 'humanitair': 708, 'export': 709, 'lhôteller': 710, 'architectural': 711, 'situat': 712, 'auvergn': 713, 'théâtr': 714, 'agrocampus': 715, 'fonctionnel': 716, 'pmepm': 717, 'nanoscient': 718, 'text': 719, 'jeux': 720, 'limmobili': 721, 'unilasall': 722, 'esip': 723, 'hydraul': 724, 'enjeux': 725, 'ebusiness': 726, '5': 727, 'français': 728, 'utt': 729, 'sh': 730, 'écotoxicolog': 731, 'delectron': 732, 'francoallemand': 733, 'régul': 734, 'complémentair': 735, 'heiisaisen': 736, 'réhabilit': 737, 'parisv': 738, 'neuroscient': 739, 'ecotoxicolog': 740, 'auxiliair': 741, 'horticol': 742, 'cibl': 743, 'bc2t': 744, 'domainedroitéconomiegest': 745, 'dériv': 746, 'cycl': 747, '13': 748, 'pont': 749, 'métall': 750, 'intern': 751, 'clientel': 752, 'mulhous': 753, 'radioprotect': 754, 'rénov': 755, 'dunivers': 756, 'lantiqu': 757, 'armé': 758, 'équestr': 759, 'marketingvent': 760, 'etabl': 761, 'fonder': 762, 'bioproced': 763, 'œuvr': 764, 'all': 765, 'sp': 766, 'aid': 767, 'zon': 768, 'céram': 769, 'verr': 770, 'simul': 771, 'dinfrastructur': 772, 'solidar': 773, 'industrialis': 774, 'stockag': 775, 'entrepreneur': 776, 'cartograph': 777, 'entraîn': 778, 'danim': 779, 'concepteurtric': 780, 'insat': 781, 'géoressourc': 782, 'franchecomt': 783, 'hydrograph': 784, 'vision': 785, 'packaging': 786, 'mathstic': 787, 'doper': 788, 'enscbp': 789, 'théolog': 790, 'dinnov': 791, 'test': 792, 'musicolog': 793, 'atc': 794, 'kw': 795, 'domot': 796, 'représent': 797, 'lanim': 798, 'navig': 799, 'médiéval': 800, 'ancien': 801, 'aérospatial': 802, 'esthet': 803, 'intellectuel': 804, 'durgenc': 805, 'support': 806, 'model': 807, 'métallurg': 808, 'prnt': 809, 'ten': 810, 'webdesign': 811, 'lingenieur': 812, 'socioculturel': 813, 'tertiair': 814, 'eea': 815, 'continu': 816, 'rout': 817, 'plurimédi': 818, 'douvrag': 819, 'froid': 820, 'mathemat': 821, 'frigorif': 822, 'drug': 823, 'val': 824, 'ski': 825, 'natur': 826, 'marqu': 827, 'déconom': 828, 'cao': 829, 'entretien': 830, 'domainescient': 831, 'process': 832, 'géorisqu': 833, 'lhabit': 834, 'conseillerer': 835, 'détect': 836, 'bioactiv': 837, 'propriet': 838, 'ensiacet': 839, 'collabor': 840, 'nouveau': 841, 'ouest': 842, 'san': 843, 'fil': 844, 'actuel': 845, 'adjoint': 846, 'réunion': 847, 'tous': 848, 'chang': 849, 'laiti': 850, '2000': 851, 'intercultural': 852, 'lois': 853, 'sciencestechnolog': 854, 'secrétair': 855, 'convers': 856, 'voil': 857, 'psychopatholog': 858, 'marnelavall': 859, 'interfac': 860, 'webmast': 861, 'diplom': 862, 'promot': 863, 'plant': 864, 'las': 865, 'motocycl': 866, 'medic': 867, 'nanomatérial': 868, 'médecin': 869, 'éditorial': 870, 'ensai': 871, 'sectoriel': 872, 'luminy': 873, 'aixmarseil': 874, 'of': 875, 'chinois': 876, 'entrain': 877, 'llce': 878, 'meti': 879, 'machin': 880, 'lasall': 881, 'terrestr': 882, 'flux': 883, 'spectroscop': 884, 'rugby': 885, 'praticien': 886, 'forg': 887, 'list': 888, 'applique': 889, 'llc': 890, 'accessoir': 891, 'tech': 892, 'créteil': 893, 'climatis': 894, 'bretagnesud': 895, 'parachut': 896, 'ladministr': 897, 'entraîneur': 898, 'parfum': 899, 'rédacteur': 900, 'full': 901, 'disposit': 902, 'acheteur': 903, 'orient': 904, 'hydrogéolog': 905, 'fusion': 906, 'dénerg': 907, 'hockey': 908, 'ingenieur': 909, 'dge': 910, 'protocol': 911, 'outil': 912, 'immunolog': 913, 'lautonom': 914, 'minister': 915, 'subatom': 916, 'monteur': 917, 'menuisi': 918, 'rp': 919, 'dinstall': 920, 'esi': 921, 'technologiessant': 922, 'administrateurtric': 923, 'hautsdefr': 924, 'economic': 925, 'beauv': 926, 'delectrotechn': 927, 'deurecom': 928, 'ile': 929, 'yncré': 930, 'bioingénier': 931, 'compiegn': 932, 'offic': 933, 'lecam': 934, 'élabor': 935, 'transit': 936, 'béton': 937, 'continental': 938, 'oeuvr': 939, 'socioéduc': 940, 'informatis': 941, 'dimag': 942, 'dadministr': 943, 'lhomm': 944, 'paléontolog': 945, 'pêch': 946, 'vendeur': 947, 'lélectr': 948, 'continus': 949, 'bioproduct': 950, 'scen': 951, 'distanc': 952, 'cuisin': 953, 'portuair': 954, 'vidéo': 955, 'physiquechim': 956, 'délectr': 957, 'métal': 958, 'habitat': 959, 'avion': 960, 'dauphin': 961, 'qse': 962, 'telecom': 963, 'caractéris': 964, 'club': 965, 'voix': 966, 'objet': 967, 'bibliothécair': 968, 'plaisanc': 969, 'agroindustr': 970, 'chien': 971, 'cris': 972, 'germanophon': 973, 'arôm': 974, 'agroéquip': 975, 'lou': 976, 'brogl': 977, 'passerel': 978, 'reproduct': 979, 'evalu': 980, 'circuit': 981, 'soin': 982, 'leurop': 983, 'angevin': 984, 'gymniqu': 985, 'dentretien': 986, 'esitc': 987, 'approfond': 988, 'géosciencesplanet': 989, 'maîtr': 990, 'vert': 991, 'nautism': 992, 'laction': 993, 'dagronom': 994, 'nord': 995, 'li': 996, 'german': 997, 'veil': 998, 'sudalsac': 999, 'rayon': 1000, 'soudag': 1001, 'traçabl': 1002, 'choix': 1003, 'esirem': 1004, 'eadd': 1005, 'esc': 1006, 'conférenci': 1007, 'cosmétolog': 1008, 'échang': 1009, 'linstrument': 1010, 'asur': 1011, 'rhônealp': 1012, 'isaralyon': 1013, 'intranetinternet': 1014, 'économiegest': 1015, 'stu': 1016, 'cloud': 1017, 'picard': 1018, 'bancair': 1019, 'llcer': 1020, 'istas': 1021, 'intranet': 1022, 'pro': 1023, 'mission': 1024, 'muséolog': 1025, 'rechercheprofessionnel': 1026, 'modelis': 1027, 'combust': 1028, 'catalys': 1029, 'vid': 1030, 'uga': 1031, 'dassist': 1032, 'rest': 1033, 'jour': 1034, 'étrangèresspécial': 1035, 'lunion': 1036, 'portug': 1037, 'immatériel': 1038, 'supervis': 1039, 'formateur': 1040, 'developp': 1041, 'ethnolog': 1042, 'virtuel': 1043, 'yvelin': 1044, 'isty': 1045, 'superviseur': 1046, 'boisson': 1047, 'légal': 1048, 'criminolog': 1049, 'notarial': 1050, 'epidémiolog': 1051, 'mpce': 1052, 'cergy': 1053, 'nordpasdecal': 1054, 'ar': 1055, 'darm': 1056, 'bienêtr': 1057, 'charpenti': 1058, 'managérial': 1059, 'lead': 1060, 'caq': 1061, 'graphist': 1062, 'bureau': 1063, 'sous': 1064, 'laudiovisuel': 1065, 'circul': 1066, 'fif': 1067, 'big': 1068, 'dœuvr': 1069, 'temp': 1070, 'xii': 1071, 'prépar': 1072, '200': 1073, 'spa': 1074, 'extérieur': 1075, 'biochimiebiolog': 1076, 'genr': 1077, 'comportemental': 1078, 'microtechn': 1079, '3000': 1080, 'cal': 1081, 'lactiv': 1082, 'hydro3': 1083, 'hydrobiogéochim': 1084, 'hydropédolog': 1085, 'assain': 1086, 'tabl': 1087, 'minéral': 1088, 'ministr': 1089, 'lévénementiel': 1090, 'hispan': 1091, 'engine': 1092, 'apprentissag': 1093, 'bio': 1094, 'armement': 1095, 'xv': 1096, 'hôteli': 1097, 'lindustrieconcept': 1098, 'gestionfin': 1099, 'vitivinicol': 1100, 'approch': 1101, 'aquacultur': 1102, 'enscpb': 1103, 'proces': 1104, 'doffici': 1105, 'nantesatlant': 1106, 'onir': 1107, 'géometr': 1108, 'argil': 1109, 'projeteur': 1110, 'tout': 1111, 'routi': 1112, 'algorithm': 1113, 'stack': 1114, 'solair': 1115, 'cart': 1116, 'itiipac': 1117, 'latmospher': 1118, 'solid': 1119, 'électrotechn': 1120, 'contraint': 1121, 'securit': 1122, 'chimiephys': 1123, 'destruct': 1124, 'nanophoton': 1125, 'basketball': 1126, 'explos': 1127, 'guerr': 1128, 'monnai': 1129, 'explor': 1130, 'cci': 1131, 'iber': 1132, 'tric': 1133, 'ateli': 1134, 'contenus': 1135, 'relig': 1136, 'tuyauter': 1137, 'vivar': 1138, 'louest': 1139, 'bac': 1140, 'enscm': 1141, 'hydro': 1142, 'valdemarn': 1143, 'valeur': 1144, 'maintien': 1145, 'larchéolog': 1146, 'laccompagn': 1147, 'midipyren': 1148, 'psb': 1149, 'démarch': 1150, 'mecan': 1151, 'économetr': 1152, 'puissanc': 1153, 'librair': 1154, 'sûr': 1155, 'vocat': 1156, 'émergent': 1157, 'dexpress': 1158, 'desm': 1159, 'cnam': 1160, 'cynophil': 1161, 'microsystem': 1162, 'évènementiel': 1163, 'microbien': 1164, 'gérontolog': 1165, 'nanostructur': 1166, 'lautomobil': 1167, 'sign': 1168, 'aprèsvent': 1169, 'médiateur': 1170, 'ident': 1171, 'ensta': 1172, 'géoénerg': 1173, 'arbre': 1174, 'amer': 1175, 'orléan': 1176, 'menuiser': 1177, 'locéan': 1178, 'importexport': 1179, 'sociétal': 1180, 'précis': 1181, 'minestelecom': 1182, 'fond': 1183, 'cpe': 1184, 'certif': 1185, 'tôler': 1186, 'addit': 1187, 'radiocommun': 1188, 'homm': 1189, 'navir': 1190, 'canoëkayak': 1191, 'biologieinformatiquebioinformat': 1192, 'bib': 1193, 'enfanc': 1194, 'cf': 1195, 'mutat': 1196, 'method': 1197, 'hydroprotech': 1198, 'écrit': 1199, 'station': 1200, 'pollut': 1201, 'manufactur': 1202, 'ecp': 1203, 'badminton': 1204, 'évalu': 1205, 'lamélior': 1206, 'terroir': 1207, 'optoélectron': 1208, 'jeanfrançois': 1209, 'champollion': 1210, 'tem': 1211, 'multilingu': 1212, 'parfumer': 1213, 'délégu': 1214, 'lal': 1215, 'écologieenviron': 1216, 'métrologiequal': 1217, 'vall': 1218, 'règlement': 1219, 'propr': 1220, 'isupfer': 1221, 'montagnard': 1222, 'aix': 1223, 'réservoir': 1224, 'hospitali': 1225, 'msci': 1226, 'léonard': 1227, 'vinc': 1228, 'electrotechn': 1229, 'yncre': 1230, 'atmospher': 1231, 'mesc': 1232, 'gardien': 1233, 'lesc': 1234, 'documental': 1235, 'plasm': 1236, 'musicien': 1237, 'daménag': 1238, 'fonci': 1239, 'gis': 1240, 'reconvers': 1241, 'signalis': 1242, 'deau': 1243, 'sonor': 1244, 'gard': 1245, 'écritmatérielimmatériel': 1246, 'mrc': 1247, 'lesitech': 1248, 'egel': 1249, 'spingénier': 1250, 'essais': 1251, 'equip': 1252, 'anglaisruss': 1253, 'webmarketing': 1254, 'si': 1255, 'motoris': 1256, 'ise': 1257, 'ense': 1258, 'tir': 1259, 'advanced': 1260, 'optron': 1261, '1er': 1262, 'char': 1263, 'déven': 1264, 'savoir': 1265, 'lannion': 1266, 'déquit': 1267, 'neom': 1268, 'usinag': 1269, 'détabl': 1270, 'extract': 1271, 'sourc': 1272, 'vol': 1273, 'asr': 1274, 'raison': 1275, 'viticultur': 1276, 'compos': 1277, 'box': 1278, 'culinair': 1279, 'xiv': 1280, 'rse': 1281, 'lexcept': 1282, 'cirqu': 1283, 'mouv': 1284, 'marchandis': 1285, 'reseau': 1286, 'auditeur': 1287, 'familial': 1288, 'econometr': 1289, 'jeun': 1290, 'denisdiderot': 1291, 'contrat': 1292, 'papi': 1293, 'télédétect': 1294, 'linstrumentationd': 1295, 'accueil': 1296, 'expérient': 1297, 'rif': 1298, 'utcspécial': 1299, 'cryptolog': 1300, 'gros': 1301, 'jardin': 1302, 'démantel': 1303, 'libr': 1304, 'eil': 1305, 'ulco': 1306, 'purpan': 1307, 'biobanqu': 1308, 'is': 1309, 'lemballag': 1310, 'exact': 1311, 'subaquat': 1312, 'dinterfac': 1313, 'asi': 1314, 'luvsq': 1315, 'parissaclay': 1316, 'sigm': 1317, 'multimedi': 1318, 'expériment': 1319, 'renseign': 1320, 'ludothécair': 1321, 'institutionnel': 1322, 'economiegest': 1323, 'genet': 1324, 'diver': 1325, 'ingeco': 1326, '000': 1327, 'deb': 1328, 'astrophys': 1329, 'équin': 1330, 'concert': 1331, '3il': 1332, 'lepf': 1333, 'daptitud': 1334, 'cuisini': 1335, 'cynotechnicien': 1336, 'dhorticultur': 1337, 'horticultur': 1338, 'entre': 1339, 'qualifi': 1340, 'géochim': 1341, 'géomatérial': 1342, 'géobiolog': 1343, '3ge': 1344, 'dagenc': 1345, 'qualitésécuritéenviron': 1346, 'oro': 1347, 'spi': 1348, 'simultan': 1349, 'randon': 1350, 'parc': 1351, 'dintérieur': 1352, 'map': 1353, '500': 1354, 'itii': 1355, 'isifc': 1356, 'lorganis': 1357, 'computing': 1358, 'reprodev': 1359, 'multimodal': 1360, 'fonctionnalis': 1361, 'dorgan': 1362, 'marchand': 1363, 'letat': 1364, 'itech': 1365, 'delectr': 1366, 'tt': 1367, 'essecepsc': 1368, 'biomécan': 1369, 'bmi': 1370, 'indifférenci': 1371, 'ladour': 1372, 'champagn': 1373, 'arden': 1374, 'vigil': 1375, 'therv': 1376, 'alliag': 1377, 'ieseg': 1378, 'gescosto': 1379, 'outillag': 1380, 'dimmeubl': 1381, 'sge': 1382, 'préhistoir': 1383, 'deven': 1384, 'mult': 1385, 'yacht': 1386, 'ouvrag': 1387, 'histoiregéograph': 1388, 'interven': 1389, 'pastoral': 1390, 'natat': 1391, 'renforc': 1392, 'catégor': 1393, 'cqpi': 1394, 'altern': 1395, 'evolutionterreenvironnementclimat': 1396, 'valid': 1397, 'éthiqu': 1398, 'toxicologieenvironnementsant': 1399, 'particuli': 1400, 'daudenci': 1401, 'maquet': 1402, 'socioéconom': 1403, 'parisdauphin': 1404, 'chaudronni': 1405, 'mobili': 1406, 'engineering': 1407, 'guideconférenci': 1408, 'diétet': 1409, 'rpi': 1410, 'dalpinismeaccompagn': 1411, 'anthropis': 1412, 'agrofournitur': 1413, 'lintervent': 1414, 'typ': 1415, 'i2sa': 1416, 'bt': 1417, 'nic': 1418, 'daccompagn': 1419, 'bioingenier': 1420, 'ciblag': 1421, 'cohabl': 1422, '8': 1423, 'répar': 1424, 'opérateurtric': 1425, 'dess': 1426, 'agrobusiness': 1427, 'occitan': 1428, 'océan': 1429, 'écointerpret': 1430, 'lœuvr': 1431, 'plurimedi': 1432, 'déploi': 1433, 'procédur': 1434, 'badg': 1435, 'multicanal': 1436, 'imprimer': 1437, 'parisest': 1438, 'inpt': 1439, 'cuir': 1440, 'métreur': 1441, 'lassur': 1442, 'maison': 1443, 'connect': 1444, 'dorient': 1445, 'daéronaut': 1446, 'parisvii': 1447, 'licd': 1448, 'enfant': 1449, 'cit': 1450, 'acteur': 1451, 'dorganis': 1452, 'report': 1453, 'peintur': 1454, 'sir': 1455, 'matmec': 1456, 'protéom': 1457, 'ressourcesenviron': 1458, 'solseauxenviron': 1459, 'ecam': 1460, 'usag': 1461, 'santément': 1462, 'martial': 1463, 'eseo': 1464, 'léger': 1465, 'éolien': 1466, 'alé': 1467, 'dsga': 1468, 'pai': 1469, 'agroenvironnemental': 1470, 'euro': 1471, 'dhabill': 1472, 'esti': 1473, 'bbrt': 1474, 'interentrepris': 1475, 'cabinet': 1476, 'enscl': 1477, 'daction': 1478, 'opt': 1479, 'biosant': 1480, 'rib': 1481, 'aérostructur': 1482, 'ieapfich': 1483, 'jeu': 1484, 'fluidiqu': 1485, 'chauss': 1486, 'enpc': 1487, 'patinag': 1488, 'rennes1': 1489, 'webmestr': 1490, 'despac': 1491, 'enseirb': 1492, 'bim': 1493, 'ae': 1494, 'biothérap': 1495, 'thérap': 1496, 'discour': 1497, 'soudeur': 1498, 'aéronef': 1499, 'stochast': 1500, 'fiabl': 1501, 'anglaisallemand': 1502, 'esie': 1503, 'enquêteur': 1504, 'voyag': 1505, 'UNK': 1506}\n",
      "{'spécial': 15492, 'mention': 11528, 'scienc': 10230, 'diplôm': 5813, 'sant': 5696, 'technolog': 5671, 'ingénieur': 4847, 'gestion': 4728, 'manag': 4469, 'national': 4120, 'domain': 3726, 'droit': 3685, 'mast': 2991, 'informat': 2877, 'system': 2864, 'industriel': 2787, 'professionnel': 2687, 'gen': 2521, 'supérieur': 2391, 'social': 2368, 'méti': 2182, 'option': 2110, 'commun': 2049, 'langu': 1960, 'humain': 1959, 'lunivers': 1957, 'art': 1956, 'appliqu': 1937, 'développ': 1769, 'lecol': 1768, 'final': 1696, 'product': 1613, 'ingénier': 1609, 'linstitut': 1536, 'respons': 1509, 'econom': 1508, 'fich': 1507, 'projet': 1498, 'recherch': 1495, 'polytechn': 1471, 'économ': 1415, 'lettr': 1368, 'environ': 1336, 'matérial': 1295, 'réseau': 1287, 'international': 1273, 'sécur': 1259, 'lécol': 1239, 'biolog': 1198, 'physiqu': 1177, 'mécan': 1176, 'techniqu': 1163, 'marketing': 1122, 'lenviron': 1071, 'chim': 1071, 'qualit': 1033, 'organis': 890, 'industr': 853, 'licenc': 835, 'chef': 811, 'mathémat': 782, 'commerc': 778, 'mainten': 759, 'activ': 757, 'télécommun': 752, 'expert': 744, 'dingénieur': 743, 'numer': 741, 'universitair': 724, 'entrepris': 711, 'histoir': 706, 'dinform': 699, 'concept': 693, 'technicien': 682, 'aménag': 657, 'public': 636, 'électron': 636, 'administr': 628, 'produit': 622, 'agroalimentair': 620, 'strateg': 617, 'bât': 605, 'commercial': 591, 'durabl': 574, 'risqu': 570, 'ressourc': 568, 'partenariat': 561, 'cqp': 554, 'automat': 551, 'logiciel': 537, 'électr': 530, 'cultur': 528, 'logist': 521, 'territoir': 517, 'charg': 516, 'biotechnolog': 509, 'patrimoin': 494, 'protect': 493, 'construct': 487, 'proced': 473, 'servic': 471, 'sport': 465, 'lill': 447, 'aliment': 447, 'contrôl': 438, 'linform': 436, 'conservatoir': 425, 'polit': 424, 'chimiqu': 421, 'traval': 420, 'affair': 419, 'culturel': 418, 'étranger': 417, 'énerget': 405, 'énerg': 400, 'civil': 393, 'civilis': 385, 'terr': 384, 'assist': 376, 'géograph': 374, 'litii': 366, 'innov': 359, 'valoris': 350, 'littératur': 349, 'anim': 346, 'multimédi': 342, 'publiqu': 342, 'dan': 340, 'inform': 337, 'agent': 336, 'paris': 335, 'format': 334, 'analys': 331, 'priv': 330, 'léduc': 329, 'lindustr': 327, 'bordeau': 318, 'digital': 318, 'pharmaceut': 316, 'tourism': 314, 'agronom': 313, 'polytech': 312, 'degr': 310, 'sportiv': 308, 'electron': 308, 'toulous': 307, 'européen': 300, 'détud': 299, 'programm': 294, 'lingénieur': 292, 'financ': 291, 'daffair': 279, 'instrument': 275, 'integr': 274, 'dentrepris': 274, 'perform': 273, 'étud': 270, 'climat': 269, 'conseil': 265, 'opérationnel': 263, 'societ': 251, 'trait': 250, 'sportif': 249, 'méthod': 249, 'franc': 248, 'montpelli': 247, 'ecol': 245, 'parcour': 243, 'transport': 243, 'psycholog': 239, 'grand': 237, 'jeuness': 233, 'statist': 229, 'ren': 227, 'energ': 224, 'littoral': 223, 'spécialis': 223, 'don': 223, 'naturel': 222, 'vent': 216, 'création': 215, 'brevet': 213, 'paysag': 212, 'transform': 212, 'design': 212, 'eau': 211, 'populair': 211, 'vi': 211, 'avanc': 210, 'group': 208, 'lenseign': 208, 'titr': 207, 'web': 205, 'développeur': 205, 'etranger': 204, 'person': 204, 'prévent': 202, 'territorial': 201, 'lyon': 201, 'espac': 200, 'etud': 199, 'concepteur': 198, 'alimentair': 198, 'electr': 198, 'bois': 194, 'strasbourg': 193, 'mod': 192, 'végétal': 191, 'lentrepris': 191, 'oper': 191, 'structur': 190, 'lorrain': 189, 'embarqu': 188, 'rural': 185, 'premi': 183, 'aéronaut': 183, 'artist': 182, 'centr': 181, 'min': 180, 'hygien': 178, 'conduit': 178, 'lénerg': 178, 'modélis': 177, 'achat': 176, 'grenobl': 175, 'intelligent': 174, 'intervent': 173, 'mécatron': 173, 'textil': 172, 'bretagn': 172, 'commercialis': 172, 'déquip': 171, 'environnemental': 169, 'sciencestechnologiessant': 169, 'composit': 168, 'ouvri': 167, 'sociolog': 166, 'leau': 165, 'financi': 165, 'dart': 165, 'urban': 164, 'médiat': 160, 'enseign': 159, 'cellulair': 159, 'lart': 158, 'nutrit': 156, 'complex': 156, 'régional': 156, 'directeur': 155, 'travail': 154, 'loisir': 154, 'réalis': 152, 'robot': 151, 'médi': 151, 'relat': 150, 'archéolog': 149, 'microbiolog': 149, 'philosoph': 149, 'mesur': 147, 'côt': 145, 'urbain': 145, 'automatis': 144, 'detat': 141, 'imag': 141, 'éduc': 140, 'bien': 139, 'gestionnair': 139, 'univers': 138, 'install': 138, 'renouvel': 138, 'maîtris': 138, '1': 138, 'nant': 136, 'distribu': 136, 'agricol': 136, 'écol': 135, 'collect': 134, 'nucléair': 133, 'physiolog': 133, 'médic': 132, 'mer': 132, 'comptabl': 131, 'mati': 131, 'analyt': 129, 'dinformat': 129, 'meilleur': 129, 'linnov': 128, 'viv': 127, 'moléculair': 127, 'audit': 123, 'délectron': 123, 'pratiqu': 123, 'miag': 123, 'ergonom': 122, 'général': 121, 'journal': 120, 'sanitair': 120, 'mobil': 120, 'haut': 120, 'ecolog': 120, 'langag': 118, 'convent': 118, 'droiteconomiegest': 117, 'écolog': 116, 'rouen': 116, 'linformat': 116, 'processus': 114, 'criminel': 114, 'planet': 114, 'mond': 114, 'restaur': 113, 'nouvel': 113, 'architectur': 113, 'expertis': 113, 'energet': 113, 'pluritechn': 112, 'télécom': 111, 'jurid': 111, 'minestélécom': 110, 'limog': 108, 'métrolog': 107, 'optiqu': 106, 'patholog': 106, 'deuxiem': 106, 'milieux': 105, 'march': 105, 'plastiqu': 105, 'dynam': 104, 'mis': 104, 'dexploit': 104, 'thermiqu': 104, 'ii': 103, 'fondamental': 103, 'biodivers': 103, 'acoust': 103, 'cognit': 102, 'stap': 101, 'linternational': 101, 'marin': 100, 'équip': 100, 'sorbon': 99, 'tourist': 99, 'form': 98, 'optimis': 98, 'coordon': 98, 'internet': 98, 'interculturel': 98, 'business': 97, 'conducteur': 97, 'i': 96, 'entrepreneuriat': 96, 'associ': 95, 'animal': 94, 'bas': 94, 'local': 94, 'spectacl': 94, 'scientif': 94, 'décis': 93, 'diagnostic': 93, 'formul': 93, 'géolog': 93, 'limag': 92, 'laliment': 92, 'secteur': 92, 'dazur': 91, 'jurist': 91, 'global': 90, 'central': 89, 'audiovisuel': 89, 'graphiqu': 88, 'savoi': 88, 'écosystem': 87, 'st': 87, 'anglais': 87, 'coordin': 87, 'vis': 86, 'daixmarseil': 86, 'dappliqu': 86, 'génom': 86, 'clermontferrand': 85, 'loir': 85, 'moniteur': 85, 'btp': 85, 'bioinformat': 84, 'linguist': 84, 'immobili': 84, 'pierr': 84, 'plasturg': 84, 'mécanicien': 84, 'poiti': 83, 'cliniqu': 83, 'document': 82, 'laménag': 82, 'grad': 82, 'educ': 81, 'dirig': 81, 'adapt': 81, 'bcpp': 81, 'maritim': 80, 'musiqu': 80, 'mar': 80, 'espagnol': 79, 'génet': 79, 'institu': 79, 'hôteller': 78, 'dactiv': 78, 'sûret': 78, 'biochim': 77, 'automobil': 77, 'action': 77, 'lélectron': 77, 'cur': 77, 'chambéry': 77, 'documentair': 77, 'cooper': 76, 'meef': 76, 'direct': 76, 'allemand': 76, 'dat': 76, 'leduc': 75, 'géomat': 74, 'livr': 74, 'connaiss': 74, 'géoscienc': 74, 'class': 74, 'artificiel': 74, 'forêt': 74, 'second': 73, 'vill': 73, 'déchet': 73, 'client': 72, 'intérieur': 72, 'léconom': 72, 'toxicolog': 71, 'anthropolog': 71, 'cybersécur': 71, 'lign': 71, 'modern': 71, 'thérapeut': 70, 'italien': 70, 'carri': 69, 'pay': 69, 'specialit': 69, 'ingenier': 69, 'physicochim': 69, 'solidair': 69, 'biomédical': 69, 'caen': 68, 'négoci': 68, 'signal': 68, 'chain': 68, 'contemporain': 68, 'and': 67, 'tour': 67, 'informationcommun': 67, 'moteur': 67, 'gouvern': 67, 'organ': 67, 'assur': 67, 'dépollu': 66, 'agricultur': 65, 'chanti': 65, 'b': 65, 'véhicul': 65, 'tvrn': 65, 'solut': 65, 'calcul': 65, 'lédit': 65, 'spatial': 65, 'rédact': 64, 'lagricultur': 64, 'biologiesant': 64, 'habill': 64, 'ordin': 64, 'traduct': 64, 'judiciair': 64, 'bibliothequ': 64, 'médical': 64, 'incend': 64, '2': 63, 'marseil': 63, 'dintervent': 63, 'laboratoir': 63, 'dales': 63, 'encadr': 62, 'troy': 62, 'dunit': 61, 'consult': 61, 'sit': 60, 'pétroli': 60, 'polyvalent': 60, 'technicocommercial': 60, 'iledefr': 59, 'banqu': 59, 'perfection': 58, 'in': 58, 'cathol': 58, 'montagn': 58, 'dhydraul': 58, 'décisionnel': 58, 'milieu': 57, 'didact': 57, 'établ': 57, 'compar': 57, 'expérimental': 57, 'pilotag': 57, 'human': 57, 'exploit': 57, 'vin': 56, 'cosmet': 56, 'visuel': 56, 'saintétien': 56, 'supply': 56, 'dou': 56, 'domainesciencestechnologiessant': 55, 'fabriqu': 55, 'analyst': 55, 'school': 55, 'aquat': 54, 'disciplin': 54, 'attach': 54, 'moyen': 54, 'luniver': 54, 'coach': 54, 'dun': 54, 'lutt': 54, 'pétrol': 53, 'architect': 53, 'ecommerc': 53, 'dagricultur': 53, 'detud': 53, 'pénal': 53, 'prof': 53, 'dessin': 53, 'dorléan': 53, 'dopal': 53, 'pyrotechn': 52, 'atlant': 52, 'petit': 52, 'capitain': 52, 'po': 52, 'organisationnel': 51, 'infrastructur': 51, 'confer': 51, 'surveil': 51, 'nancy': 51, 'lux': 51, 'lenerg': 51, 'certificat': 50, 'a': 50, 'chaîn': 50, 'polymer': 50, 'brest': 50, 'interact': 50, 'clermont': 50, '3': 49, 'capteur': 49, 'foresti': 49, 'méditerran': 49, 'command': 49, 'compétent': 49, 'glac': 49, 'efficac': 48, 'stpe': 48, 'amélior': 48, 'ecoconcept': 48, 'cadr': 48, 'belfortmontbéliard': 47, 'danalys': 47, 'molécul': 47, '3d': 47, 'naval': 47, 'contr': 47, 'fiscal': 46, 'transfrontali': 46, 'institut': 46, 'microélectron': 46, 'maintenicien': 46, 'etou': 46, 'interpret': 46, 'sinistr': 46, 'accompagn': 45, 'cqpm': 45, 'propuls': 45, 'synthes': 45, 'docu': 45, 'pau': 45, 'sol': 44, 'europ': 44, 'cellul': 44, 'man': 44, 'physiopatholog': 44, 'sud': 44, 'ecosystem': 44, 'danger': 44, 'reim': 44, 'délectrotechn': 44, 'détat': 43, 'fonction': 43, 'pme': 43, 'ferroviair': 43, 'chaudronner': 43, 'lespac': 42, 'quantit': 42, 'évolu': 42, 'enspm': 42, 'bioscienc': 42, 'géotechn': 42, 'quart': 42, 'diagnostiqueur': 42, 'matériel': 42, 'conserv': 41, 'dingénier': 41, 'agroscient': 41, 'décor': 41, 'biomolécul': 41, 'laéronaut': 41, 'lurban': 41, 'lingénier': 41, 'nautiqu': 41, 'saintetien': 41, 'silico': 41, 'infograph': 40, 'topograph': 40, 'lagroalimentair': 40, 'nanotechnolog': 40, 'sciencestechnologiesant': 40, 'dijon': 40, 'histor': 40, 'défens': 40, 'bioindustr': 40, 'paysager': 40, 'lénerget': 40, 'fili': 40, 'unit': 40, 'evolu': 40, 'miash': 40, 'répart': 39, 'firm': 39, 'vibrat': 39, 'denseign': 39, 'événementiel': 39, 'anglophon': 39, 'inp': 39, 'aérien': 39, 'condition': 39, 'handicap': 38, 'xi': 38, 'surfac': 38, 'cinem': 38, 'non': 38, 'archiv': 38, 'agrosystem': 37, 'classiqu': 37, 'mentionméti': 37, 'e': 37, 'pharmacolog': 37, 'géophys': 37, 'consomm': 37, 'alsac': 37, 'vieil': 37, 'vétérinair': 37, 'imager': 37, 'cheff': 37, 'peintr': 36, 'diffus': 36, 'daccueil': 36, 'guid': 36, 'rapproch': 36, 'ameubl': 36, 'gaz': 36, 'médicosocial': 36, 'aquitain': 36, 'directeurtric': 36, 'spiritu': 36, 'lensilensc': 36, 'golf': 36, 'linternet': 35, 'russ': 35, 'facteur': 35, 'ensib': 35, 'écoconcept': 35, 'air': 35, 'fluid': 35, 'contenti': 35, 'patrimonial': 34, 'photon': 34, 'littérair': 34, 'édit': 34, 'cy': 34, 'communic': 34, 'nuisanc': 34, 'créateur': 34, 'pilot': 34, 'secour': 34, 'contrôleur': 34, 'popul': 34, 'agroressourc': 34, 'material': 34, 'dateli': 34, 'lanalys': 33, 'bm': 33, 'iii': 33, 'football': 33, 'emballag': 33, 'emploi': 33, 'enseeiht': 33, 'théor': 33, 'humanitair': 33, 'export': 33, 'lhôteller': 33, 'architectural': 33, 'situat': 33, 'auvergn': 33, 'théâtr': 32, 'agrocampus': 32, 'fonctionnel': 32, 'pmepm': 32, 'nanoscient': 32, 'text': 32, 'jeux': 32, 'limmobili': 32, 'unilasall': 32, 'esip': 32, 'hydraul': 32, 'enjeux': 32, 'ebusiness': 32, '5': 32, 'français': 32, 'utt': 32, 'sh': 32, 'écotoxicolog': 31, 'delectron': 31, 'francoallemand': 31, 'régul': 31, 'complémentair': 31, 'heiisaisen': 31, 'réhabilit': 31, 'parisv': 31, 'neuroscient': 31, 'ecotoxicolog': 31, 'auxiliair': 31, 'horticol': 30, 'cibl': 30, 'bc2t': 30, 'domainedroitéconomiegest': 30, 'dériv': 30, 'cycl': 30, '13': 30, 'pont': 30, 'métall': 30, 'intern': 30, 'clientel': 30, 'mulhous': 30, 'radioprotect': 30, 'rénov': 30, 'dunivers': 30, 'lantiqu': 30, 'armé': 30, 'équestr': 29, 'marketingvent': 29, 'etabl': 29, 'fonder': 29, 'bioproced': 29, 'œuvr': 29, 'all': 29, 'sp': 29, 'aid': 29, 'zon': 29, 'céram': 29, 'verr': 29, 'simul': 29, 'dinfrastructur': 29, 'solidar': 29, 'industrialis': 29, 'stockag': 29, 'entrepreneur': 29, 'cartograph': 28, 'entraîn': 28, 'danim': 28, 'concepteurtric': 28, 'insat': 28, 'géoressourc': 28, 'franchecomt': 28, 'hydrograph': 28, 'vision': 28, 'packaging': 28, 'mathstic': 28, 'doper': 28, 'enscbp': 28, 'théolog': 28, 'dinnov': 28, 'test': 28, 'musicolog': 28, 'atc': 28, 'kw': 28, 'domot': 28, 'représent': 27, 'lanim': 27, 'navig': 27, 'médiéval': 27, 'ancien': 27, 'aérospatial': 27, 'esthet': 27, 'intellectuel': 27, 'durgenc': 27, 'support': 27, 'model': 27, 'métallurg': 27, 'prnt': 27, 'ten': 27, 'webdesign': 26, 'lingenieur': 26, 'socioculturel': 26, 'tertiair': 26, 'eea': 26, 'continu': 26, 'rout': 26, 'plurimédi': 26, 'douvrag': 26, 'froid': 26, 'mathemat': 26, 'frigorif': 26, 'drug': 26, 'val': 26, 'ski': 26, 'natur': 26, 'marqu': 26, 'déconom': 25, 'cao': 25, 'entretien': 25, 'domainescient': 25, 'process': 25, 'géorisqu': 25, 'lhabit': 25, 'conseillerer': 25, 'détect': 25, 'bioactiv': 25, 'propriet': 25, 'ensiacet': 25, 'collabor': 25, 'nouveau': 24, 'ouest': 24, 'san': 24, 'fil': 24, 'actuel': 24, 'adjoint': 24, 'réunion': 24, 'tous': 24, 'chang': 24, 'laiti': 24, '2000': 24, 'intercultural': 24, 'lois': 24, 'sciencestechnolog': 24, 'secrétair': 24, 'convers': 24, 'voil': 24, 'psychopatholog': 24, 'marnelavall': 24, 'interfac': 24, 'webmast': 24, 'diplom': 23, 'promot': 23, 'plant': 23, 'las': 23, 'motocycl': 23, 'medic': 23, 'nanomatérial': 23, 'médecin': 23, 'éditorial': 23, 'ensai': 23, 'sectoriel': 23, 'luminy': 23, 'aixmarseil': 23, 'of': 23, 'chinois': 23, 'entrain': 23, 'llce': 23, 'meti': 23, 'machin': 23, 'lasall': 23, 'terrestr': 23, 'flux': 23, 'spectroscop': 23, 'rugby': 23, 'praticien': 23, 'forg': 23, 'list': 22, 'applique': 22, 'llc': 22, 'accessoir': 22, 'tech': 22, 'créteil': 22, 'climatis': 22, 'bretagnesud': 22, 'parachut': 22, 'ladministr': 22, 'entraîneur': 22, 'parfum': 22, 'rédacteur': 22, 'full': 22, 'disposit': 22, 'acheteur': 22, 'orient': 22, 'hydrogéolog': 22, 'fusion': 22, 'dénerg': 22, 'hockey': 22, 'ingenieur': 22, 'dge': 22, 'protocol': 22, 'outil': 22, 'immunolog': 22, 'lautonom': 21, 'minister': 21, 'subatom': 21, 'monteur': 21, 'menuisi': 21, 'rp': 21, 'dinstall': 21, 'esi': 21, 'technologiessant': 21, 'administrateurtric': 21, 'hautsdefr': 21, 'economic': 21, 'beauv': 21, 'delectrotechn': 21, 'deurecom': 21, 'ile': 21, 'yncré': 21, 'bioingénier': 21, 'compiegn': 21, 'offic': 21, 'lecam': 21, 'élabor': 21, 'transit': 21, 'béton': 21, 'continental': 21, 'oeuvr': 21, 'socioéduc': 20, 'informatis': 20, 'dimag': 20, 'dadministr': 20, 'lhomm': 20, 'paléontolog': 20, 'pêch': 20, 'vendeur': 20, 'lélectr': 20, 'continus': 20, 'bioproduct': 20, 'scen': 20, 'distanc': 20, 'cuisin': 20, 'portuair': 20, 'vidéo': 20, 'physiquechim': 20, 'délectr': 20, 'métal': 20, 'habitat': 20, 'avion': 20, 'dauphin': 20, 'qse': 20, 'telecom': 20, 'caractéris': 20, 'club': 20, 'voix': 20, 'objet': 19, 'bibliothécair': 19, 'plaisanc': 19, 'agroindustr': 19, 'chien': 19, 'cris': 19, 'germanophon': 19, 'arôm': 19, 'agroéquip': 19, 'lou': 19, 'brogl': 19, 'passerel': 19, 'reproduct': 19, 'evalu': 19, 'circuit': 19, 'soin': 19, 'leurop': 19, 'angevin': 19, 'gymniqu': 19, 'dentretien': 19, 'esitc': 19, 'approfond': 19, 'géosciencesplanet': 19, 'maîtr': 19, 'vert': 19, 'nautism': 19, 'laction': 19, 'dagronom': 19, 'nord': 19, 'li': 19, 'german': 19, 'veil': 19, 'sudalsac': 19, 'rayon': 19, 'soudag': 19, 'traçabl': 18, 'choix': 18, 'esirem': 18, 'eadd': 18, 'esc': 18, 'conférenci': 18, 'cosmétolog': 18, 'échang': 18, 'linstrument': 18, 'asur': 18, 'rhônealp': 18, 'isaralyon': 18, 'intranetinternet': 18, 'économiegest': 18, 'stu': 18, 'cloud': 18, 'picard': 18, 'bancair': 18, 'llcer': 18, 'istas': 18, 'intranet': 18, 'pro': 18, 'mission': 18, 'muséolog': 18, 'rechercheprofessionnel': 18, 'modelis': 18, 'combust': 18, 'catalys': 18, 'vid': 18, 'uga': 18, 'dassist': 18, 'rest': 18, 'jour': 18, 'étrangèresspécial': 18, 'lunion': 18, 'portug': 18, 'immatériel': 18, 'supervis': 18, 'formateur': 18, 'developp': 18, 'ethnolog': 18, 'virtuel': 18, 'yvelin': 18, 'isty': 18, 'superviseur': 18, 'boisson': 18, 'légal': 17, 'criminolog': 17, 'notarial': 17, 'epidémiolog': 17, 'mpce': 17, 'cergy': 17, 'nordpasdecal': 17, 'ar': 17, 'darm': 17, 'bienêtr': 17, 'charpenti': 17, 'managérial': 17, 'lead': 17, 'caq': 17, 'graphist': 17, 'bureau': 17, 'sous': 17, 'laudiovisuel': 17, 'circul': 17, 'fif': 17, 'big': 17, 'dœuvr': 17, 'temp': 17, 'xii': 17, 'prépar': 17, '200': 17, 'spa': 17, 'extérieur': 17, 'biochimiebiolog': 17, 'genr': 17, 'comportemental': 17, 'microtechn': 17, '3000': 17, 'cal': 17, 'lactiv': 17, 'hydro3': 17, 'hydrobiogéochim': 17, 'hydropédolog': 17, 'assain': 17, 'tabl': 17, 'minéral': 17, 'ministr': 17, 'lévénementiel': 17, 'hispan': 17, 'engine': 17, 'apprentissag': 17, 'bio': 17, 'armement': 17, 'xv': 17, 'hôteli': 17, 'lindustrieconcept': 16, 'gestionfin': 16, 'vitivinicol': 16, 'approch': 16, 'aquacultur': 16, 'enscpb': 16, 'proces': 16, 'doffici': 16, 'nantesatlant': 16, 'onir': 16, 'géometr': 16, 'argil': 16, 'projeteur': 16, 'tout': 16, 'routi': 16, 'algorithm': 16, 'stack': 16, 'solair': 16, 'cart': 16, 'itiipac': 16, 'latmospher': 16, 'solid': 16, 'électrotechn': 16, 'contraint': 16, 'securit': 16, 'chimiephys': 16, 'destruct': 16, 'nanophoton': 16, 'basketball': 16, 'explos': 16, 'guerr': 16, 'monnai': 16, 'explor': 16, 'cci': 16, 'iber': 16, 'tric': 16, 'ateli': 16, 'contenus': 16, 'relig': 16, 'tuyauter': 16, 'vivar': 16, 'louest': 15, 'bac': 15, 'enscm': 15, 'hydro': 15, 'valdemarn': 15, 'valeur': 15, 'maintien': 15, 'larchéolog': 15, 'laccompagn': 15, 'midipyren': 15, 'psb': 15, 'démarch': 15, 'mecan': 15, 'économetr': 15, 'puissanc': 15, 'librair': 15, 'sûr': 15, 'vocat': 15, 'émergent': 15, 'dexpress': 15, 'desm': 15, 'cnam': 15, 'cynophil': 15, 'microsystem': 15, 'évènementiel': 15, 'microbien': 15, 'gérontolog': 15, 'nanostructur': 15, 'lautomobil': 15, 'sign': 15, 'aprèsvent': 15, 'médiateur': 15, 'ident': 15, 'ensta': 15, 'géoénerg': 15, 'arbre': 15, 'amer': 15, 'orléan': 15, 'menuiser': 15, 'locéan': 15, 'importexport': 15, 'sociétal': 15, 'précis': 15, 'minestelecom': 15, 'fond': 15, 'cpe': 15, 'certif': 15, 'tôler': 15, 'addit': 15, 'radiocommun': 15, 'homm': 15, 'navir': 15, 'canoëkayak': 15, 'biologieinformatiquebioinformat': 14, 'bib': 14, 'enfanc': 14, 'cf': 14, 'mutat': 14, 'method': 14, 'hydroprotech': 14, 'écrit': 14, 'station': 14, 'pollut': 14, 'manufactur': 14, 'ecp': 14, 'badminton': 14, 'évalu': 14, 'lamélior': 14, 'terroir': 14, 'optoélectron': 14, 'jeanfrançois': 14, 'champollion': 14, 'tem': 14, 'multilingu': 14, 'parfumer': 14, 'délégu': 14, 'lal': 14, 'écologieenviron': 14, 'métrologiequal': 14, 'vall': 14, 'règlement': 14, 'propr': 14, 'isupfer': 14, 'montagnard': 14, 'aix': 14, 'réservoir': 14, 'hospitali': 14, 'msci': 14, 'léonard': 14, 'vinc': 14, 'electrotechn': 14, 'yncre': 14, 'atmospher': 14, 'mesc': 14, 'gardien': 14, 'lesc': 14, 'documental': 14, 'plasm': 14, 'musicien': 14, 'daménag': 14, 'fonci': 14, 'gis': 14, 'reconvers': 14, 'signalis': 14, 'deau': 14, 'sonor': 14, 'gard': 14, 'écritmatérielimmatériel': 14, 'mrc': 14, 'lesitech': 14, 'egel': 14, 'spingénier': 14, 'essais': 14, 'equip': 14, 'anglaisruss': 14, 'webmarketing': 14, 'si': 14, 'motoris': 14, 'ise': 14, 'ense': 14, 'tir': 14, 'advanced': 14, 'optron': 14, '1er': 14, 'char': 14, 'déven': 13, 'savoir': 13, 'lannion': 13, 'déquit': 13, 'neom': 13, 'usinag': 13, 'détabl': 13, 'extract': 13, 'sourc': 13, 'vol': 13, 'asr': 13, 'raison': 13, 'viticultur': 13, 'compos': 13, 'box': 13, 'culinair': 13, 'xiv': 13, 'rse': 13, 'lexcept': 13, 'cirqu': 13, 'mouv': 13, 'marchandis': 13, 'reseau': 13, 'auditeur': 13, 'familial': 13, 'econometr': 13, 'jeun': 13, 'denisdiderot': 13, 'contrat': 13, 'papi': 13, 'télédétect': 13, 'linstrumentationd': 13, 'accueil': 13, 'expérient': 13, 'rif': 13, 'utcspécial': 13, 'cryptolog': 13, 'gros': 13, 'jardin': 13, 'démantel': 13, 'libr': 13, 'eil': 13, 'ulco': 13, 'purpan': 13, 'biobanqu': 13, 'is': 13, 'lemballag': 13, 'exact': 13, 'subaquat': 13, 'dinterfac': 13, 'asi': 13, 'luvsq': 13, 'parissaclay': 13, 'sigm': 13, 'multimedi': 13, 'expériment': 13, 'renseign': 13, 'ludothécair': 13, 'institutionnel': 13, 'economiegest': 13, 'genet': 13, 'diver': 13, 'ingeco': 13, '000': 13, 'deb': 13, 'astrophys': 13, 'équin': 13, 'concert': 12, '3il': 12, 'lepf': 12, 'daptitud': 12, 'cuisini': 12, 'cynotechnicien': 12, 'dhorticultur': 12, 'horticultur': 12, 'entre': 12, 'qualifi': 12, 'géochim': 12, 'géomatérial': 12, 'géobiolog': 12, '3ge': 12, 'dagenc': 12, 'qualitésécuritéenviron': 12, 'oro': 12, 'spi': 12, 'simultan': 12, 'randon': 12, 'parc': 12, 'dintérieur': 12, 'map': 12, '500': 12, 'itii': 12, 'isifc': 12, 'lorganis': 12, 'computing': 12, 'reprodev': 12, 'multimodal': 12, 'fonctionnalis': 12, 'dorgan': 12, 'marchand': 12, 'letat': 12, 'itech': 12, 'delectr': 12, 'tt': 12, 'essecepsc': 12, 'biomécan': 12, 'bmi': 12, 'indifférenci': 12, 'ladour': 12, 'champagn': 12, 'arden': 12, 'vigil': 12, 'therv': 12, 'alliag': 12, 'ieseg': 12, 'gescosto': 12, 'outillag': 12, 'dimmeubl': 12, 'sge': 12, 'préhistoir': 12, 'deven': 12, 'mult': 12, 'yacht': 12, 'ouvrag': 12, 'histoiregéograph': 12, 'interven': 12, 'pastoral': 12, 'natat': 12, 'renforc': 12, 'catégor': 12, 'cqpi': 12, 'altern': 12, 'evolutionterreenvironnementclimat': 12, 'valid': 12, 'éthiqu': 12, 'toxicologieenvironnementsant': 12, 'particuli': 12, 'daudenci': 12, 'maquet': 12, 'socioéconom': 12, 'parisdauphin': 12, 'chaudronni': 12, 'mobili': 12, 'engineering': 12, 'guideconférenci': 12, 'diétet': 12, 'rpi': 12, 'dalpinismeaccompagn': 12, 'anthropis': 12, 'agrofournitur': 12, 'lintervent': 12, 'typ': 11, 'i2sa': 11, 'bt': 11, 'nic': 11, 'daccompagn': 11, 'bioingenier': 11, 'ciblag': 11, 'cohabl': 11, '8': 11, 'répar': 11, 'opérateurtric': 11, 'dess': 11, 'agrobusiness': 11, 'occitan': 11, 'océan': 11, 'écointerpret': 11, 'lœuvr': 11, 'plurimedi': 11, 'déploi': 11, 'procédur': 11, 'badg': 11, 'multicanal': 11, 'imprimer': 11, 'parisest': 11, 'inpt': 11, 'cuir': 11, 'métreur': 11, 'lassur': 11, 'maison': 11, 'connect': 11, 'dorient': 11, 'daéronaut': 11, 'parisvii': 11, 'licd': 11, 'enfant': 11, 'cit': 11, 'acteur': 11, 'dorganis': 11, 'report': 11, 'peintur': 11, 'sir': 11, 'matmec': 11, 'protéom': 11, 'ressourcesenviron': 11, 'solseauxenviron': 11, 'ecam': 11, 'usag': 11, 'santément': 11, 'martial': 11, 'eseo': 11, 'léger': 11, 'éolien': 11, 'alé': 11, 'dsga': 11, 'pai': 11, 'agroenvironnemental': 11, 'euro': 11, 'dhabill': 11, 'esti': 11, 'bbrt': 11, 'interentrepris': 11, 'cabinet': 11, 'enscl': 11, 'daction': 11, 'opt': 11, 'biosant': 11, 'rib': 11, 'aérostructur': 11, 'ieapfich': 11, 'jeu': 11, 'fluidiqu': 11, 'chauss': 11, 'enpc': 11, 'patinag': 11, 'rennes1': 11, 'webmestr': 11, 'despac': 11, 'enseirb': 11, 'bim': 11, 'ae': 11, 'biothérap': 11, 'thérap': 11, 'discour': 11, 'soudeur': 11, 'aéronef': 11, 'stochast': 11, 'fiabl': 11, 'anglaisallemand': 11, 'esie': 11, 'enquêteur': 11, 'voyag': 11, 'UNK': 0}\n"
     ]
    }
   ],
   "source": [
    "voc, word_counts = vocabulary([' '.join(text) for text in train_texts_clean], count_threshold=10, voc_threshold=0)\n",
    "print(voc)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb7810f",
   "metadata": {},
   "source": [
    "What do you think is the **appropriate vocabulary size here** ? Would any further pre-processing make sense ? Motivate your answer.\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd0ba21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1507\n"
     ]
    }
   ],
   "source": [
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f7b7d",
   "metadata": {},
   "source": [
    "## 2 - Symbolic text representations\n",
    "\n",
    "We can use the ```CountVectorizer``` class from scikit-learn to obtain the first set of representations:\n",
    "- Use the appropriate argument to get your own vocabulary\n",
    "- Fit the vectorizer on your training data, transform your test data\n",
    "- Create a ```LogisticRegression``` model and train it with these representations. Display the confusion matrix using functions from ```sklearn.metrics``` \n",
    "\n",
    "Then, re-execute the same pipeline with the ```TfidfVectorizer```.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1601c205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.268871925360475\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "1-environnement       0.25      0.23      0.24      1209\n",
      "      2-defense       0.33      0.10      0.15       639\n",
      "   3-patrimoine       0.26      0.11      0.15       389\n",
      "     4-economie       0.29      0.15      0.19       178\n",
      "    5-recherche       0.34      0.14      0.20       163\n",
      "     6-nautisme       0.25      0.10      0.14       272\n",
      " 7-aeronautique       0.18      0.06      0.09       347\n",
      "     8-securite       0.23      0.29      0.25      1146\n",
      "   9-multimedia       0.41      0.15      0.22       124\n",
      " 10-humanitaire       0.37      0.16      0.23       370\n",
      "   11-nucleaire       0.28      0.64      0.39      1957\n",
      "     12-enfance       0.21      0.08      0.11       505\n",
      "  13-saisonnier       0.20      0.07      0.10       462\n",
      "  14-assistance       0.23      0.14      0.18       802\n",
      "       15-sport       0.28      0.15      0.19       148\n",
      "  16-ingenierie       0.32      0.25      0.28       721\n",
      "\n",
      "       accuracy                           0.27      9432\n",
      "      macro avg       0.28      0.18      0.20      9432\n",
      "   weighted avg       0.27      0.27      0.24      9432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer + Logistic Regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "plot_flag = False\n",
    "\n",
    "pipline_counterLR = Pipeline([\n",
    "    ('countervec', CountVectorizer(vocabulary=[word.lower() for word in voc])),\n",
    "    ('model', \n",
    "     LogisticRegression(\n",
    "         max_iter=1000, \n",
    "         multi_class='ovr', \n",
    "         solver='liblinear', \n",
    "         random_state=42, \n",
    "         fit_intercept=True, \n",
    "         penalty='l1',\n",
    "         C=0.19938842909500154\n",
    "        ))\n",
    "])\n",
    "pipline_counterLR.fit([' '.join(text) for text in train_texts_clean], train_labels)\n",
    "predictions = pipline_counterLR.predict([' '.join(text) for text in test_texts_clean])\n",
    "print('Accuracy:', accuracy_score(test_labels, predictions))\n",
    "print(classification_report(test_labels, predictions, target_names=Categories))\n",
    "\n",
    "if plot_flag:\n",
    "    cm = confusion_matrix(test_labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=Categories)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ea3bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2737489397794741\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "1-environnement       0.24      0.28      0.26      1209\n",
      "      2-defense       0.39      0.10      0.16       639\n",
      "   3-patrimoine       0.23      0.07      0.11       389\n",
      "     4-economie       0.32      0.18      0.23       178\n",
      "    5-recherche       0.37      0.09      0.14       163\n",
      "     6-nautisme       0.18      0.06      0.09       272\n",
      " 7-aeronautique       0.15      0.05      0.07       347\n",
      "     8-securite       0.22      0.31      0.26      1146\n",
      "   9-multimedia       0.57      0.06      0.12       124\n",
      " 10-humanitaire       0.37      0.19      0.25       370\n",
      "   11-nucleaire       0.30      0.63      0.40      1957\n",
      "     12-enfance       0.24      0.07      0.10       505\n",
      "  13-saisonnier       0.28      0.10      0.15       462\n",
      "  14-assistance       0.25      0.16      0.19       802\n",
      "       15-sport       0.31      0.11      0.17       148\n",
      "  16-ingenierie       0.31      0.26      0.28       721\n",
      "\n",
      "       accuracy                           0.27      9432\n",
      "      macro avg       0.30      0.17      0.19      9432\n",
      "   weighted avg       0.28      0.27      0.24      9432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pipline_tfidf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(vocabulary=[word.lower() for word in voc])),\n",
    "    ('model', \n",
    "     LogisticRegression(\n",
    "         max_iter=1000, \n",
    "         multi_class='ovr', \n",
    "         solver='saga', \n",
    "         random_state=42, \n",
    "         fit_intercept=True, \n",
    "         penalty='l2',\n",
    "         C=0.2021867460428368\n",
    "        ))\n",
    "])\n",
    "pipline_tfidf.fit([' '.join(text) for text in train_texts_clean], train_labels)\n",
    "predictions = pipline_tfidf.predict([' '.join(text) for text in test_texts_clean])\n",
    "print('Accuracy:', accuracy_score(test_labels, predictions))\n",
    "print(classification_report(test_labels, predictions, target_names=Categories))\n",
    "\n",
    "\n",
    "if plot_flag:\n",
    "    cm = confusion_matrix(test_labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=Categories)\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b2bcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameters to search\n",
    "#     penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "#     solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "#     C = trial.suggest_float('C', 0.1, 5.0)\n",
    "    \n",
    "#     # Create pipeline\n",
    "#     pipeline = Pipeline([\n",
    "#         ('countervec', TfidfVectorizer(\n",
    "#             vocabulary=[word.lower() for word in voc]\n",
    "#         )),\n",
    "#         ('model', LogisticRegression(\n",
    "#             max_iter=2000,\n",
    "#             multi_class='ovr',\n",
    "#             solver=solver,\n",
    "#             random_state=42,\n",
    "#             penalty=penalty,\n",
    "#             C=C\n",
    "#         ))\n",
    "#     ])\n",
    "    \n",
    "#     # Fit pipeline\n",
    "#     pipeline.fit([' '.join(text) for text in train_texts_clean], train_labels)\n",
    "    \n",
    "#     # Predict and calculate accuracy\n",
    "#     predictions = pipeline.predict([' '.join(text) for text in test_texts_clean])\n",
    "#     accuracy = accuracy_score(test_labels, predictions)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Create Optuna study and optimize\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # Print the best parameters found\n",
    "# print(f\"Best trial: {study.best_trial.params}\")\n",
    "# print(f\"Best accuracy: {study.best_trial.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed387bc",
   "metadata": {},
   "source": [
    "## 3 - Dense Representations from Topic Modeling\n",
    "\n",
    "Now, the goal is to re-use the bag-of-words representations we obtained earlier - but reduce their dimension through a **topic model**. Note that this allows to obtain reduced **document representations**, which we can again use directly to perform classification.\n",
    "- Do this with two models: ```TruncatedSVD``` and ```LatentDirichletAllocation```\n",
    "- Pick $300$ as the dimensionality of the latent representation (*i.e*, the number of topics)\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4204cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2675996607294317\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "1-environnement       0.24      0.26      0.25      1209\n",
      "      2-defense       0.34      0.10      0.16       639\n",
      "   3-patrimoine       0.24      0.08      0.12       389\n",
      "     4-economie       0.26      0.15      0.19       178\n",
      "    5-recherche       0.38      0.13      0.20       163\n",
      "     6-nautisme       0.17      0.10      0.13       272\n",
      " 7-aeronautique       0.14      0.04      0.07       347\n",
      "     8-securite       0.23      0.31      0.26      1146\n",
      "   9-multimedia       0.40      0.10      0.16       124\n",
      " 10-humanitaire       0.33      0.20      0.25       370\n",
      "   11-nucleaire       0.30      0.60      0.40      1957\n",
      "     12-enfance       0.20      0.07      0.11       505\n",
      "  13-saisonnier       0.20      0.07      0.11       462\n",
      "  14-assistance       0.24      0.17      0.20       802\n",
      "       15-sport       0.26      0.11      0.15       148\n",
      "  16-ingenierie       0.29      0.25      0.27       721\n",
      "\n",
      "       accuracy                           0.27      9432\n",
      "      macro avg       0.26      0.17      0.19      9432\n",
      "   weighted avg       0.26      0.27      0.24      9432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TruncatedSVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_topics = 300\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=[word.lower() for word in voc])\n",
    "train_texts_tfidf = vectorizer.fit_transform([' '.join(text) for text in train_texts_clean])\n",
    "test_texts_tfidf = vectorizer.transform([' '.join(text) for text in test_texts_clean])\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "train_texts_svd = svd.fit_transform(train_texts_tfidf)\n",
    "test_texts_svd = svd.transform(test_texts_tfidf)\n",
    "\n",
    "# Logistic Regression\n",
    "model = LogisticRegression(\n",
    "         max_iter=1000, \n",
    "         multi_class='ovr', \n",
    "         solver='liblinear', \n",
    "         random_state=42, \n",
    "         fit_intercept=True, \n",
    "         penalty='l2',\n",
    "         C=0.5351208229170836\n",
    "        )\n",
    "model.fit(train_texts_svd, train_labels)\n",
    "predictions = model.predict(test_texts_svd)\n",
    "print('Accuracy:', accuracy_score(test_labels, predictions))\n",
    "print(classification_report(test_labels, predictions, target_names=Categories))\n",
    "\n",
    "if plot_flag:\n",
    "    cm = confusion_matrix(test_labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=Categories)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a4eafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26569126378286684\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "1-environnement       0.24      0.27      0.25      1209\n",
      "      2-defense       0.38      0.09      0.15       639\n",
      "   3-patrimoine       0.23      0.07      0.10       389\n",
      "     4-economie       0.30      0.15      0.20       178\n",
      "    5-recherche       0.35      0.07      0.12       163\n",
      "     6-nautisme       0.14      0.04      0.07       272\n",
      " 7-aeronautique       0.13      0.03      0.05       347\n",
      "     8-securite       0.23      0.32      0.27      1146\n",
      "   9-multimedia       0.54      0.06      0.10       124\n",
      " 10-humanitaire       0.31      0.16      0.21       370\n",
      "   11-nucleaire       0.29      0.64      0.39      1957\n",
      "     12-enfance       0.21      0.05      0.08       505\n",
      "  13-saisonnier       0.20      0.06      0.09       462\n",
      "  14-assistance       0.25      0.15      0.19       802\n",
      "       15-sport       0.30      0.09      0.14       148\n",
      "  16-ingenierie       0.31      0.23      0.27       721\n",
      "\n",
      "       accuracy                           0.27      9432\n",
      "      macro avg       0.27      0.16      0.17      9432\n",
      "   weighted avg       0.26      0.27      0.23      9432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "train_texts_lda = lda.fit_transform(train_texts_tfidf)\n",
    "test_texts_lda = lda.transform(test_texts_tfidf)\n",
    "\n",
    "# Logistic Regression\n",
    "model = LogisticRegression(\n",
    "         max_iter=1000, \n",
    "         multi_class='ovr', \n",
    "         solver='saga', \n",
    "         random_state=42, \n",
    "         fit_intercept=True, \n",
    "         penalty='l2',\n",
    "         C=0.2021867460428368\n",
    "        )\n",
    "model.fit(train_texts_svd, train_labels)\n",
    "predictions = model.predict(test_texts_svd)\n",
    "print('Accuracy:', accuracy_score(test_labels, predictions))\n",
    "print(classification_report(test_labels, predictions, target_names=Categories))\n",
    "\n",
    "if plot_flag:\n",
    "    cm = confusion_matrix(test_labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=Categories)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893c948",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "We picked $300$ as number of topics. What would be the procedure to follow if we wanted to choose this hyperparameter through the data ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c105a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60f74d75",
   "metadata": {},
   "source": [
    "## 4 - Dense Count-based Representations\n",
    "\n",
    "The following function allows to obtain very large-dimensional vectors for **words**. We will now follow a different procedure:\n",
    "- Step 1: Obtain the co-occurence matrix, based on the vocabulary, giving you a vector by word in the vocabulary.\n",
    "- Step 2: Apply an SVD to obtain **word embeddings** of dimension $300$, for each word in the vocabulary.\n",
    "- Step 3: Obtain document representations by aggregating embeddings associated to each word in the document.\n",
    "- Step 4: Train a classifier on the (document representations, label) pairs. \n",
    "\n",
    "Some instructions:\n",
    "- In step 1, use the ```co_occurence_matrix``` function, which you need to complete.\n",
    "- In step 2, use ```TruncatedSVD```to obtain word representations of dimension $300$ from the output of the ```co_occurence_matrix``` function.\n",
    "- In step 3, use the ```sentence_representations``` function, which you will need to complete.\n",
    "- In step 4, put the pipeline together by obtaining document representations for both training and testing data. Careful: the word embeddings must come from the *training data co-occurence matrix* only.\n",
    "\n",
    "Lastly, add a **Step 1b**: transform the co-occurence matrix into the PPMI matrix, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05ed10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurence_matrix(corpus, vocabulary, window=0):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        vocabulary (dictionary): words to use in the matrix\n",
    "        window (int): size of the context window; when 0, the context is the whole sentence\n",
    "    Returns:\n",
    "        matrix (array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input    \n",
    "    \"\"\" \n",
    "    l = len(vocabulary)\n",
    "    M = np.zeros((l,l))\n",
    "    Vocab_idx = {word: i for i, word in enumerate(vocabulary)}\n",
    "    for sent in corpus:\n",
    "        # Get the sentence\n",
    "        sent =  sent.split()\n",
    "\n",
    "        sent_idx =  [Vocab_idx.get(word, l-1) for word in sent]\n",
    "        # Avoid one-word sentences - can create issues in normalization: \n",
    "        if len(sent_idx) == 1:\n",
    "                sent_idx.append(len(vocabulary)-1) # This adds an Unkown word to the sentence\n",
    "        # Go through the indexes and add 1 / dist(i,j) to M[i,j] if words of index i and j appear in the same window\n",
    "        for i, idx_i in enumerate(sent_idx):\n",
    "            if window > 0:\n",
    "                # Window context: max and min are used to stay within sentence boundaries\n",
    "                l_ctx_idx = sent_idx[max(0, i-window):min(len(sent_idx), i + window + 1)]\n",
    "            else:\n",
    "                # Full sentence context\n",
    "                l_ctx_idx = sent_idx\n",
    "\n",
    "            # Update the matrix for left context\n",
    "            for ctx_idx in l_ctx_idx:\n",
    "                M[idx_i][ctx_idx] += 1\n",
    "                M[ctx_idx][idx_i] += 1  # because co-occurrence is a symmetric relation\n",
    "       \n",
    "    return M  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be497b3",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e843cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[      0.       0.       0. ...       0.       0.       0.]\n",
      " [      0.    6088.       0. ...       0.       0.   21404.]\n",
      " [      0.       0.       0. ...       0.       0.       0.]\n",
      " ...\n",
      " [      0.       0.       0. ...       0.       0.       0.]\n",
      " [      0.       0.       0. ...       0.       0.       0.]\n",
      " [      0.   21404.       0. ...       0.       0. 3163506.]]\n"
     ]
    }
   ],
   "source": [
    "# Obtain the co-occurence matrix, transform it as needed, reduce its dimension\n",
    "co_occurance_Matrix =  co_occurence_matrix(train_texts, voc, window=2)\n",
    "print(co_occurance_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58160c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 2.14449907e+04  5.94591226e+03  1.40814678e+02 ...  3.30398578e-44\n",
      "   1.44399085e-45 -1.71076694e-44]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 3.16364777e+06 -3.95570210e+01 -1.85338664e+01 ...  5.55614841e-42\n",
      "   7.42584183e-43 -1.27236139e-42]]\n"
     ]
    }
   ],
   "source": [
    "n_topics = 300\n",
    "svd = TruncatedSVD(n_components=n_topics)\n",
    "X_topics = svd.fit_transform(co_occurance_Matrix)\n",
    "print(X_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312da12",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e413841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_representations(texts, vocabulary, embeddings, np_func=np.mean):\n",
    "    \"\"\"\n",
    "    Represent the sentences as a combination of the vector of its words.\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : a list of sentences   \n",
    "    vocabulary : dict\n",
    "        From words to indexes of vector.\n",
    "    embeddings : Matrix containing word representations\n",
    "    np_func : function (default: np.sum)\n",
    "        A numpy matrix operation that can be applied columnwise, \n",
    "        like `np.mean`, `np.sum`, or `np.prod`. \n",
    "    Returns\n",
    "    -------\n",
    "    np.array, dimension `(len(texts), embeddings.shape[1])`            \n",
    "    \"\"\"\n",
    "    representations = []\n",
    "    for text in texts:\n",
    "        indexes = [vocabulary.get(word) for word in text.split() if word in vocabulary]\n",
    "        # Indexes of words in the sentence obtained thanks to the vocabulary\n",
    "        if indexes:\n",
    "            sentrep = np_func([embeddings[idx] for idx in indexes], axis=0)\n",
    "        else:\n",
    "            sentrep = np.zeros(embeddings.shape[1])\n",
    "        \n",
    "        # Embeddings of words in the sentence, aggregated thanks to the function\n",
    "        representations.append(sentrep)\n",
    "    representations = np.array(representations)    \n",
    "    return representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2462a0a2",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5256e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "1-environnement       0.26      0.03      0.05      1181\n",
      "      2-defense       0.38      0.01      0.02       604\n",
      "   3-patrimoine       0.00      0.00      0.00       413\n",
      "     4-economie       0.17      0.02      0.04       171\n",
      "    5-recherche       0.29      0.01      0.02       155\n",
      "     6-nautisme       0.40      0.01      0.01       266\n",
      " 7-aeronautique       0.07      0.00      0.01       368\n",
      "     8-securite       0.28      0.10      0.15      1171\n",
      "   9-multimedia       0.00      0.00      0.00       132\n",
      " 10-humanitaire       0.00      0.00      0.00       405\n",
      "   11-nucleaire       0.20      0.94      0.34      1880\n",
      "     12-enfance       0.00      0.00      0.00       516\n",
      "  13-saisonnier       0.18      0.02      0.04       485\n",
      "  14-assistance       0.00      0.00      0.00       776\n",
      "       15-sport       0.00      0.00      0.00       166\n",
      "  16-ingenierie       0.29      0.03      0.05       743\n",
      "\n",
      "       accuracy                           0.21      9432\n",
      "      macro avg       0.16      0.07      0.05      9432\n",
      "   weighted avg       0.19      0.21      0.10      9432\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Obtain document representations, apply the classifier\n",
    "doc_representation = sentence_representations(texts_reduced, voc, X_topics, np_func=np.mean)\n",
    "\n",
    "X_train_doc, X_test_doc, y_train_doc, y_test_doc = train_test_split(doc_representation, labels_reduced, test_size=0.2, random_state=108)\n",
    "\n",
    "model.fit(X_train_doc, y_train_doc)\n",
    "y_pred_doc = model.predict(X_test_doc)\n",
    "\n",
    "print(classification_report(y_test_doc, y_pred_doc,target_names=Categories))\n",
    "\n",
    "if plot_flag:\n",
    "    cm = confusion_matrix(y_test_doc, y_pred_doc)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=Categories)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9dbec5",
   "metadata": {},
   "source": [
    "## 5 - Dense Prediction-based Representations\n",
    "\n",
    "We will now use word embeddings from ```Word2Vec```: which we will train ourselves\n",
    "\n",
    "We will use the ```gensim``` library for its implementation of word2vec in python. Since we want to keep the same vocabulary as before: we'll first create the model, then re-use the vocabulary we generated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80339bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2383776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(vector_size=300,\n",
    "                 window=5,\n",
    "                 null_word=len(word_counts))\n",
    "model.build_vocab_from_freq(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48e7ec",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "895599f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is to be trained with a list of tokenized sentences, containing the full training dataset.\n",
    "preprocessed_corpus = [nltk.word_tokenize(text) for text in train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "157ba2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(376059, 13757400)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(preprocessed_corpus, total_examples=len(preprocessed_corpus), epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e1daa",
   "metadata": {},
   "source": [
    "Then, we can re-use the ```sentence_representations```function like before to obtain document representations, and apply classification. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66123a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_representation = sentence_representations(texts_reduced, voc, model.wv.vectors, np_func=np.mean)\n",
    "\n",
    "X_train_doc, X_test_doc, y_train_doc, y_test_doc = train_test_split(doc_representation, labels_reduced, test_size=0.2, random_state=108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a18b961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.20653095843935537\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "1-environnement       0.22      0.02      0.04      1181\n",
      "      2-defense       0.00      0.00      0.00       604\n",
      "   3-patrimoine       0.00      0.00      0.00       413\n",
      "     4-economie       0.15      0.02      0.04       171\n",
      "    5-recherche       0.29      0.01      0.02       155\n",
      "     6-nautisme       0.00      0.00      0.00       266\n",
      " 7-aeronautique       0.00      0.00      0.00       368\n",
      "     8-securite       0.28      0.09      0.14      1171\n",
      "   9-multimedia       0.00      0.00      0.00       132\n",
      " 10-humanitaire       0.00      0.00      0.00       405\n",
      "   11-nucleaire       0.20      0.95      0.33      1880\n",
      "     12-enfance       0.00      0.00      0.00       516\n",
      "  13-saisonnier       0.33      0.00      0.00       485\n",
      "  14-assistance       0.00      0.00      0.00       776\n",
      "       15-sport       0.00      0.00      0.00       166\n",
      "  16-ingenierie       0.24      0.02      0.03       743\n",
      "\n",
      "       accuracy                           0.21      9432\n",
      "      macro avg       0.11      0.07      0.04      9432\n",
      "   weighted avg       0.15      0.21      0.09      9432\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "model = LogisticRegression(\n",
    "         max_iter=1000, \n",
    "         multi_class='ovr', \n",
    "         solver='liblinear', \n",
    "         random_state=42, \n",
    "         fit_intercept=True, \n",
    "         penalty='l1'\n",
    "        )\n",
    "model.fit(X_train_doc, y_train_doc)\n",
    "predictions = model.predict(X_test_doc)\n",
    "print('Accuracy:', accuracy_score(y_test_doc, predictions))\n",
    "print(classification_report(y_test_doc, predictions, target_names=Categories))\n",
    "\n",
    "if plot_flag:\n",
    "    cm = confusion_matrix(y_test_doc, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=Categories)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55816893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-04-16 11:49:23,500] A new study created in memory with name: no-name-92d9d40a-813b-4efd-97bf-2119a70dfde1\n",
      "[I 2024-04-16 11:49:40,904] Trial 0 finished with value: 0.20716709075487702 and parameters: {'penalty': 'l2', 'solver': 'liblinear', 'C': 1.2482645022620387}. Best is trial 0 with value: 0.20716709075487702.\n",
      "[I 2024-04-16 11:50:02,330] Trial 1 finished with value: 0.20695504664970313 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 2.1698504806038006}. Best is trial 0 with value: 0.20716709075487702.\n",
      "[I 2024-04-16 11:50:19,212] Trial 2 finished with value: 0.20695504664970313 and parameters: {'penalty': 'l2', 'solver': 'liblinear', 'C': 0.9422682318257666}. Best is trial 0 with value: 0.20716709075487702.\n",
      "[I 2024-04-16 11:50:27,133] Trial 3 finished with value: 0.20621289228159456 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.6660196380090323}. Best is trial 0 with value: 0.20716709075487702.\n",
      "[I 2024-04-16 11:50:50,210] Trial 4 finished with value: 0.2079092451229856 and parameters: {'penalty': 'l2', 'solver': 'liblinear', 'C': 3.847184206592421}. Best is trial 4 with value: 0.2079092451229856.\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2024-04-16 13:03:15,890] Trial 5 finished with value: 0.20780322307039864 and parameters: {'penalty': 'l1', 'solver': 'saga', 'C': 3.2139671627522404}. Best is trial 4 with value: 0.2079092451229856.\n",
      "[I 2024-04-16 13:10:46,128] Trial 6 finished with value: 0.20780322307039864 and parameters: {'penalty': 'l2', 'solver': 'saga', 'C': 2.0710527082616816}. Best is trial 4 with value: 0.2079092451229856.\n",
      "[I 2024-04-16 13:17:03,532] Trial 7 finished with value: 0.2079092451229856 and parameters: {'penalty': 'l2', 'solver': 'saga', 'C': 1.6788298300369788}. Best is trial 4 with value: 0.2079092451229856.\n",
      "[I 2024-04-16 13:17:34,973] Trial 8 finished with value: 0.2079092451229856 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 3.0968438447403366}. Best is trial 4 with value: 0.2079092451229856.\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2024-04-16 14:30:23,537] Trial 9 finished with value: 0.20801526717557253 and parameters: {'penalty': 'l1', 'solver': 'saga', 'C': 4.648156714465925}. Best is trial 9 with value: 0.20801526717557253.\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2024-04-16 15:44:08,447] Trial 10 finished with value: 0.2082273112807464 and parameters: {'penalty': 'l1', 'solver': 'saga', 'C': 4.952631841401947}. Best is trial 10 with value: 0.2082273112807464.\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2024-04-16 16:57:25,805] Trial 11 finished with value: 0.2082273112807464 and parameters: {'penalty': 'l1', 'solver': 'saga', 'C': 4.902696066698738}. Best is trial 10 with value: 0.2082273112807464.\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\GAALOK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to search\n",
    "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "    C = trial.suggest_float('C', 0.1, 5.0)\n",
    "    model = LogisticRegression(\n",
    "         max_iter=2000, \n",
    "         multi_class='ovr', \n",
    "         solver=solver,\n",
    "         random_state=42, \n",
    "         fit_intercept=True, \n",
    "         penalty=penalty,\n",
    "         C=C\n",
    "        )\n",
    "    model.fit(X_train_doc, y_train_doc)\n",
    "    predictions = model.predict(X_test_doc)\n",
    "    # print('Accuracy:', accuracy_score(y_test_doc, predictions))\n",
    "    # print(classification_report(y_test_doc, predictions, target_names=Categories))\n",
    "    # # Create pipeline\n",
    "    # pipeline = Pipeline([\n",
    "    #     ('countervec', TfidfVectorizer(\n",
    "    #         vocabulary=[word.lower() for word in voc]\n",
    "    #     )),\n",
    "    #     ('model', LogisticRegression(\n",
    "    #         max_iter=2000,\n",
    "    #         multi_class='ovr',\n",
    "    #         solver=solver,\n",
    "    #         random_state=42,\n",
    "    #         penalty=penalty,\n",
    "    #         C=C\n",
    "    #     ))\n",
    "    # ])\n",
    "    \n",
    "    # # Fit pipeline\n",
    "    # pipeline.fit([' '.join(text) for text in train_texts_clean], train_labels)\n",
    "    \n",
    "    # # Predict and calculate accuracy\n",
    "    # predictions = pipeline.predict([' '.join(text) for text in test_texts_clean])\n",
    "    # accuracy = accuracy_score(test_labels, predictions)\n",
    "    \n",
    "    return accuracy_score(y_test_doc, predictions)\n",
    "\n",
    "# Create Optuna study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n",
    "print(f\"Best accuracy: {study.best_trial.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21842b8",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Comment on the results. What is the big issue with the dataset that using embeddings did not solve ? \n",
    "**Given this type of data**, what would you propose if you needed solve this task (i.e, reach a reasonnable performance) in an industrial context ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c778f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
